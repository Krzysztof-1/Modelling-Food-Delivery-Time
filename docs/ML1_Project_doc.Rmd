---
title: "A Data-Driven Approach to Predicting Food Delivery Time: A Multi-Model Machine Learning Framework"
author: "Krzysztof Baran, Curdin Caderas, Wenxing Xu"
date: "2025-05-07"
bibliography: references.bib
csl: apa.csl
output: 
    html_document:
      toc: true
      toc_float: true
      toc_depth: 3
      code_folding: hide
    df_print: paged
knitr: 
  opts_chunk:
    warning:false
    message:false
---

# 1. Introduction

In recent years, the rapid expansion of online food delivery services has transformed the way consumers access meals, especially in densely populated urban areas. Ensuring timely delivery is essential not only for customer satisfaction but also for maintaining competitive advantage in a highly saturated market. However, predicting food delivery times is inherently complex, influenced by factors ranging from geographic location and traffic patterns to weather conditions and courier characteristics. This project leverages data-driven methods to address this challenge by modeling delivery duration using a range of machine learning techniques.

The primary objective of this project is to build predictive models that accurately estimate food delivery times based on a rich set of features describing the delivery context. Specifically, we aim to:

- Apply multiple machine learning models to predict delivery time.

- Evaluate model performance using appropriate metrics.

- Interpret feature importance to understand the key drivers of delivery speed.

Accurate delivery time predictions can significantly enhance operational efficiency for food delivery platforms. From optimizing route planning to improving user experience through reliable ETAs, these models have practical implications for both logistics teams and end customers. Moreover, the insights gained from the modeling process can inform strategic decisions such as resource allocation, staffing during peak hours, and pricing policies.

# 2. Data Description

## 2.1 Data Source
The dataset used in this project originates from a real-world scenario involving food delivery logistics. It is publicly available on Kaggle @gibin2023food. This dataset captures a diverse and comprehensive set of variables related to food delivery orders, making it an ideal candidate for exploring machine learning approaches to predict delivery times.

**Dataset Overview**
The dataset contains 10,001 delivery records, each representing a unique food delivery transaction. These records encompass various dimensions of the delivery process, including geographic information, courier characteristics, weather and traffic conditions, and order-specific features. The data schema is outlined as follows:

- ID: A unique identifier for each delivery instance.

- Delivery_person_ID: Unique ID for the delivery person, with a city code prefix (e.g., "DEL" for Delhi).

- Delivery_person_Age: Age of the delivery person in years.

- Delivery_person_Ratings: Customer-provided rating of the delivery person (typically on a 5-point scale).

- Restaurant_latitude / Restaurant_longitude: GPS coordinates of the restaurant.

- Delivery_location_latitude / Delivery_location_longitude: GPS coordinates of the customer delivery address.

- Type_of_order: Categorical variable describing the type of food (e.g., meal, snacks, drinks, buffet).

- Type_of_vehicle: Type of vehicle used for delivery (e.g., scooter, motorcycle, cycle, electric scooter).

- Temperature: Ambient temperature at the time of delivery (°C).

- Humidity: Relative humidity (%) during delivery.

- Weather_description: Textual description of weather (e.g., "sunny", "cloudy", "stormy").

- Traffic_Level: Categorical description of traffic congestion (e.g., "very low", "low", "moderate", "high", "very high").

- Distance (km): Computed distance between restaurant and delivery location in kilometers.

- TARGET: Delivery time in minutes (target variable for prediction).

## 2.2 Data Cleaning and Exploratory Data Analysis

Before starting any exploratory data analysis, we decided to modify the column names in the raw data to enhance the readability of the entire report.

```{r load_libraries, warning=FALSE, message=FALSE}
# Load required libraries
library(dplyr)
library(ggplot2)
library(ggspatial)
library(terra)
library(sf)
library(maptiles)
library(gridExtra)
library(patchwork)
library(GGally)
library(caret)
library(broom)
library(knitr)
library(e1071)
library(tidyr)
library(stringr)
library(brglm2)
library(mgcv)
library(kableExtra)
#suppressPackageStartupMessages(library(tidyverse))
#library(tidyverse)


# Load the raw data
df.food_time <- read.csv("../data/Food_Time_Data_Set.csv")

# Renaming column names to snake case (incl. more descriptive names)

df.food_time <- df.food_time %>%
  rename(
    order_id = ID,
    courier_id = Delivery_person_ID,
    courier_age_years = Delivery_person_Age,
    courier_rating_1_to_5 = Delivery_person_Ratings,
    restaurant_latitude_deg = Restaurant_latitude,
    restaurant_longitude_deg = Restaurant_longitude,
    customer_latitude_deg = Delivery_location_latitude,
    customer_longitude_deg = Delivery_location_longitude,
    order_type = Type_of_order,
    vehicle_type = Type_of_vehicle,
    temperature_celsius = temperature,
    humidity_percent = humidity,
    precipitation_mm = precipitation,
    weather_description = weather_description,
    traffic_level = Traffic_Level,
    distance_km = Distance..km.,
    delivery_time_min = TARGET
  )
```


### 2.2.1 Geographic Filtering
To gain an initial understanding of the data set, we begin by examining the geographical distribution of the delivery observations. Four columns provide relevant location information:

`restaurant_longitude_deg`, `Restaurant_longitude`, `Delivery_location_latitude`, `Delivery_location_longitude`. All delivery distances are under 80 km.

An analysis of the restaurants’ latitude and longitude values revealed a wide range: latitudes from -30 to 30 and longitudes from -80 to 80. However, the distribution is uneven, with most data points concentrated at latitudes greater than 0 and longitudes greater than 60.

To narrow the data set to the relevant geographic area, we applied a filter to retain only observations with latitude ≥ 0 and longitude ≥ 60. This refinement reduced the data set from 10,001 to 9,084 entries, all located within a single country—India.

To visualize the geographic distribution of these observations, we first defined the map boundaries using the data set’s minimum and maximum latitude and longitude values. These coordinates were converted into a simple feature object representing the corners of the bounding box. This object was then used to download map tiles corresponding to the area of interest, enabling accurate spatial visualization.

Figure 1 displays all restaurant and delivery locations overlaid on the background map defined by bounding box. The plotted points indicate that the observations were collected from 22 cities across India, including major urban centers such as New Delhi, Mumbai, Bangalore, Kolkata, and Chennai.


```{r geographic_filtering, fig.height=4, fig.align='center', warning=FALSE, message=FALSE, exec = FALSE}
# Clean the data by filtering for geographic coordinates
df.food_time.clean <- df.food_time %>%
  filter(
    customer_longitude_deg >= 60,
    restaurant_longitude_deg >= 60,
    customer_latitude_deg >= 0,
    restaurant_latitude_deg >= 0
  )

# Create bounding box using the cleaned data
bbox.sf <- st_as_sf(
  data.frame(
    Latitude = c(min(df.food_time.clean$restaurant_latitude_deg), 
                 max(df.food_time.clean$restaurant_latitude_deg)),
    Longitude = c(min(df.food_time.clean$restaurant_longitude_deg), 
                  max(df.food_time.clean$restaurant_longitude_deg))
  ),
  coords = c("Longitude", "Latitude"),
  crs = "+proj=longlat"
)

# Download map tiles
example.map <- get_tiles(bbox.sf, provider = "OpenStreetMap")

# Extract bounding box limits
bbox.limits <- st_bbox(bbox.sf)

# Figure 1: Plot restaurant and delivery locations on the map
ggplot() +
  layer_spatial(data = example.map) +
  geom_point(data = df.food_time.clean,
             aes(x = restaurant_longitude_deg, y = restaurant_latitude_deg),
             color = "blue", size = 2, alpha = 0.7, shape = 16) +
  geom_point(data = df.food_time.clean,
             aes(x = customer_longitude_deg, y = customer_latitude_deg),
             color = "red", size = 2, alpha = 0.7, shape = 16) +
  coord_sf(xlim = bbox.limits[c(1, 3)], ylim = bbox.limits[c(2, 4)]) +
  labs(title = "Figure 1: Restaurant and Delivery Locations",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

### 2.2.2 Data Cleaning and Feature Engineering

To prepare the data set for analysis, several cleaning and transformation steps were performed:

- Conversion of Numerical Values:
The `distance_km` and `delivery_time_min` columns, originally stored as character strings with potential non-numeric characters, were cleaned using regular expressions and converted to numeric format.

- Categorical Variable Cleaning:
The `order_type` and `vehicle_type` columns were stripped of leading/trailing whitespaces and converted to factors to facilitate modeling and analysis.

- Traffic Level Standardization:
The `traffic_level` column was converted to lowercase and trimmed to remove inconsistencies, then stored as a factor variable `traffic_level_factor`.

- Weather Description Grouping:
The original `weather_description` field was cleaned and mapped into three broader weather categories:

  - `Clear`: includes terms like "clear sky" or "few clouds"

  - `Poor Visibility`: includes terms such as "fog" and "haze"

  - `Rainy`: includes various rain-related terms

These were stored as an ordered factor in a new variable:  `weather_category`.

- Long Delivery Flag:
A binary variable `long_delivery_flag` was created to indicate whether a delivery took 40 minutes or longer. This variable serves as the target for the binomial model. 

- Average Speed Calculation:
Delivery speed `average_speed_kmph` was computed in kilometers per hour by dividing distance by delivery time, and then converting to an integer. This variable serves as the target for the Poisson model. 

- Missing Data Removal:
All rows containing missing values after the above transformations were dropped to ensure data quality.

The result is a cleaned and enriched data set containing 9,035 observations, ready for exploratory data analysis and predictive modeling.


```{r data_cleaning, warning=FALSE, message=FALSE}
#This part will do the following:
#- Clean numeric values and convert to numeric
#- Clean and convert categorical variables to factors
#- Clean traffic level
#- Clean weather description & create the three categories Clear / Poor Visibility / Rainy
#- Insert a flag for long delivery time (if >= 40 min)
#- Calculate the average speed in km/h
#- Remove rows with NA values
#- Filter out coordinates which are not in India

df_clean <- df.food_time.clean %>%
  select(-X) %>%
  mutate(
    # Clean numeric values and convert to numeric
    distance_km = as.numeric(na_if(gsub("[^0-9\\.]", "", distance_km), "")),  
    delivery_time_min = as.numeric(na_if(gsub("[^0-9\\.]", "", delivery_time_min), "")),

    # Clean and convert categorical variables to factors
    order_type_factor = factor(str_trim(order_type)),
    vehicle_type_factor = factor(str_trim(vehicle_type)),

    # Clean traffic level
    traffic_level_cleaned = str_trim(tolower(traffic_level)),
    traffic_level_factor = factor(traffic_level_cleaned),

    # Clean weather description
    weather_description_cleaned = str_trim(tolower(weather_description)),
    weather_category = factor(case_when(
      weather_description_cleaned %in% c("broken clouds", "clear sky", "few clouds", "overcast clouds", "scattered clouds") ~ "Clear",
      weather_description_cleaned %in% c("fog", "haze", "smoke") ~ "Poor Visibility",
      weather_description_cleaned %in% c("mist", "moderate rain", "light rain") ~ "Rainy",
      TRUE ~ NA_character_
    ), levels = c("Clear", "Poor Visibility", "Rainy"), ordered = TRUE),

    # Flag for long delivery time
    long_delivery_flag = if_else(delivery_time_min >= 40, 1, 0),

    # Calculate average speed in km/h
    average_speed_kmph = as.integer(if_else(delivery_time_min > 0, distance_km * 60 / delivery_time_min, NA_real_))
  ) %>%
  drop_na()
```

```{r data_info, collapse=TRUE}
# Check for missing values in cleaned data & str
colSums(is.na(df_clean))
str(df_clean)
dim(df_clean)

# View the cleaned dataset
head(df_clean)
write.csv(df_clean, "../data/cleaned_data.csv", row.names = FALSE)
```

### 2.2.3 Exploratory Data Analysis

To start the EDA we will look at the distribution of our (main) target variable: `delivery_time_min`.
In order to do this we create a histogram and a boxplot. Furthermore, we look at the summary of this column. 

```{r delivery_time_eda, fig.width=12, fig.height=5, message=FALSE, warning=FALSE}
# Create histogram of target variable "Delivery Time [min]"
p1 <- ggplot(df_clean, aes(x = delivery_time_min)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
  labs(title = "Distribution of Delivery Time [min]", x = "Delivery Time (min)", y = "Count") +
  theme_minimal()

# Boxplot of target variable
p2 <- ggplot(df_clean, aes(y = delivery_time_min)) +
  geom_boxplot(fill = "skyblue") +
  labs(title = "Boxplot of Delivery Time", y = "Delivery Time (min)") +
  theme_minimal()

# Display next to each other
p1 + p2

# Summary
summary(df_clean$delivery_time_min)
```

What we can see from looking at the above plots of our target variable is that the distribution seems right-skewed. This can be seen on the histogram itself (flattening more slowly to the right) as well as in the boxplot (many outliers above the upper whisker).

Furthermore, this can be seen by looking at the summary:

- Mean > Median
- Max value is approx. four times the size of median.

In addition to our observations, we know that time is considered an "amount". During lectures we learned that it is advisable to log-transform amounts.

Following the aforementioned points, in the next step we log-transform our target variable into `log_delivery_time` and include it in the dataframe `df_cleaned`. To verify the log-transformation we will create the same plots and output as for the original target variable.

```{r log_delivery_time_eda, fig.width=12, fig.height=5, message=FALSE, warning=FALSE}
# Add log transformed delivery time to df_clean
df_clean$log_delivery_time <- log(df_clean$delivery_time_min)

# Plot Histogram of log-transformed delivery time
p3 <- ggplot(df_clean, aes(x = log_delivery_time)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  labs(title = "Histogram of Log-Transformed Delivery Time", x = "log(Delivery Time)", y = "Count") +
  theme_minimal()

# Create boxplot of log transformed delivery time
p4 <- ggplot(df_clean, aes(y = log_delivery_time)) +
  geom_boxplot(fill = "skyblue") +
  labs(title = "Boxplot of Log-Transformed Delivery Time", y = "log(Delivery Time)") +
  theme_minimal()

# Display next to each other
p3 + p4

# Summary
summary(df_clean$log_delivery_time)
```

After log-transforming our target variable, the distribution looks more symmetrical. The boxplot looks more compact and has less outliers above the upper whisker. Furthermore, we can see that mean and median are closer to each other.

Based on these observations (and the strong hint in our lecture notes), we decided to further proceed with the log-transformed target variable.

After investigating the target variable, we now check the distributions of the numeric predictors.


```{r predictor_histograms, fig.width=12, fig.height=8, message=FALSE, warning=FALSE}
# Select numeric variables for plot
numeric_vars <- df_clean %>%
  select(distance_km, courier_age_years, courier_rating_1_to_5,
         temperature_celsius, humidity_percent, precipitation_mm)

# Plot distribution of numeric variables
plots <- lapply(names(numeric_vars), function(var) {
  ggplot(df_clean, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "steelblue", color = "white") +
    labs(title = var, x = var, y = "Count") +
    theme_minimal()
})

# Arrange plots (using patchwork)
wrap_plots(plots, ncol = 3)
```

Looking at the different distributions we observe the following:

- **distance_km:** Most deliveries happen within ~ 5 - 20 km with a few extreme values beyond 40 km. This variable is right-skewed.

- **courier_age_years:** Rather uniform distribution in the range of ~20 to 40 years.

- **courier_rating_1_to_5:** Most ratings are clustered near 5. This suggests that ratings are overall high with not too much variability. It could be that there is only limited predictive power from this predictor (to be further investigated).

- **temperature_celsius:** Centered around ~22°C, symmetrical shape

- **humidity_percent:** Rather high variability. This could be due to different geographic areas.

- **precipitation_mm:** Many zero values which leads us to believe that there could be an error in the data. We will not use this predictor for further analysis.


In the next step we will investigate the numeric predictors further by looking at scatter plots of each numeric predictor on the (log-transformed) target variable.

```{r predictor_scatterplots, fig.width=12, fig.height=8, message=FALSE, warning=FALSE}

# Select numeric predictors (without precipitation_mm)
numeric_predictors <- c("distance_km", "courier_age_years", "courier_rating_1_to_5",
                        "temperature_celsius", "humidity_percent")

# Create scatter plots incl. correlation coefficient
scatter_plots <- lapply(numeric_predictors, function(var) {
  # Pearson-Korrelation berechnen (na.rm = TRUE für Sicherheit)
  cor_val <- round(cor(df_clean[[var]], df_clean$log_delivery_time, use = "complete.obs"), 2)
  
  # Create plot itself
  ggplot(df_clean, aes_string(x = var, y = "log_delivery_time")) +
    geom_point(alpha = 0.3, color = "black") +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    annotate("text", x = Inf, y = Inf, hjust = 1.1, vjust = 1.5,
             label = paste0("r = ", cor_val),
             size = 4, fontface = "italic") +
    labs(title = paste(var, "vs. Log Delivery Time"), x = var, y = "log(Delivery Time)") +
    theme_minimal()
})
# Arrange plots (using patchwork)
wrap_plots(scatter_plots, ncol = 3)
```

From looking at the plots, the correlation line and Pearson's r, we observe the following relationship with the log-transformed target variable:

- **distance_km:** seems to be a strong positive relationship (r = 0.84). i.e. the farther the delivery distance, the longer the (log-transformed) delivery time. This numeric predictor is the most important in our data set.

- **courier_age_years:** There is essentially no correlation between the courier age and (log-transformed) delivery time (r = 0.01). 

- **courier_rating_1_to_5:** there seems to be a rather weak negative relationship (r = 0.1). This could be interpreted that higher-rated couriers tend to deliver slightly faster. However the effect seems very weak and the data is clustered around high ratings which could limit the usefulness of this predictor.

- **temperature_celsius:** there seems to be a very weak positive relationship (r = 0.06), practically not meaningful.

- **humidity_percent:** There is essentially no correlation between humidity and delivery time (r = - 0.02, scattered points around flat line).

Among all numeric predictors, only `distance_km` shows a strong (positive) linear relationship with the log-transformed delivery time. All other variables exhibit weak to negligible correlations.


For out categorical predictors we start by investigating the counts in each category.
```{r predictor_bar_charts, fig.width=12, fig.height=10, message=FALSE, warning=FALSE}

# Relevel factor level in meaningful way
df_clean$traffic_level_factor <- factor(
  df_clean$traffic_level_factor,
  levels = c("very low", "low", "moderate", "high", "very high")
)

# Plot counts for categorical variables
p1 <- ggplot(df_clean, aes(x = order_type_factor, fill = order_type_factor)) +
  geom_bar() +
  labs(title = "Orders by Type", x = "Order Type", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

p2 <- ggplot(df_clean, aes(x = vehicle_type_factor, fill = vehicle_type_factor)) +
  geom_bar() +
  labs(title = "Orders by Vehicle Type", x = "Vehicle Type", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

p3 <- ggplot(df_clean, aes(x = traffic_level_factor, fill = traffic_level_factor)) +
  geom_bar() +
  labs(title = "Orders by Traffic Level", x = "Traffic Level", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

p4 <- ggplot(df_clean, aes(x = weather_category, fill = weather_category)) +
  geom_bar() +
  labs(title = "Orders by Weather", x = "Weather", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

(p1 / p2 / p3 / p4)
```

From the bar charts above, we observe the following:

- **order_type_factor:** all categories (buffer, drinks, meal, snack) are similarly frequent. There is no major imbalance. This means that our models can learn equally well across all order types.

- **vehicle_type_factor:** Motorcycles are the most preferred delivery method, followed by scooters. Electric scooters and bicycles are rare. The only few data points for non-motorized vehicles may reduce model performance, this is to be kept in mind when fitting the different models.

- **traffic_level_factor:** Most deliveries happen under moderate or high traffic. Very low traffic is the least frequent. Also here this imbalance needs to be kept in consideration when fitting the model and splitting into test / training data.

- **weather_category:** Most deliveries happen in clear weather, while poor visibility and rain occur less often. 



After looking at the counts of the categorical variables, we now look at the log-transformed delivery time versus our categorical variables using boxplots.

```{r predictor_boxplots, fig.width=12, fig.height=14, message=FALSE, warning=FALSE}

# Create boxplots on delivery time by categorical variables
b1 <- ggplot(df_clean, aes(x = order_type_factor, y = log_delivery_time, fill = order_type_factor)) +
  geom_boxplot() +
  labs(title = "Log Delivery Time by Order Type", x = "Order Type", y = "log(Delivery Time)") +
  theme_minimal() + theme(legend.position = "none")

b2 <- ggplot(df_clean, aes(x = vehicle_type_factor, y = log_delivery_time, fill = vehicle_type_factor)) +
  geom_boxplot() +
  labs(title = "Log Delivery Time by Vehicle Type", x = "Vehicle Type", y = "log(Delivery Time)") +
  theme_minimal() + theme(legend.position = "none")

b3 <- ggplot(df_clean, aes(x = traffic_level_factor, y = log_delivery_time, fill = traffic_level_factor)) +
  geom_boxplot() +
  labs(title = "Log Delivery Time by Traffic Level", x = "Traffic Level", y = "log(Delivery Time)") +
  theme_minimal() + theme(legend.position = "none")

b4 <- ggplot(df_clean, aes(x = weather_category, y = log_delivery_time, fill = weather_category)) +
  geom_boxplot() +
  labs(title = "Log Delivery Time by Weather", x = "Weather", y = "log(Delivery Time)") +
  theme_minimal() + theme(legend.position = "none")

(b1 / b2 / b3 / b4)
```

By visually inspecting the box plots we can make the following observations:

- **order_type_factor:** Order types seem not to differ too much in regards to their log-transformed delivery time. However, we can see that "drinks" take longer that the other factors. One reason could be that handling might be more difficult for drinks.

- **vehicle_type_factor:** A similar picture emerges when looking at the vehicle type. Only "bicycle" seems to deviate with having a lower median in delivery time. However, the count of bicycles in the data is very low. Moreover, these few delivery jobs where a bicycle was used were for shorter distances (not displayed in this report).

- **traffic_level_factor:** There is a clear upward trend from factor "very low" to "very high". i.e. the median of `log_delivery_time` increases with the traffic intensity. This would mean that heavier traffic leads to longer delivery times.

- **weather_category:** For the weather category "clear" the median delivery time seems to be slightly longer than for the other two categories. However, there are also many more deliveries during clear weather in our data set.

Since we have observed that traffic level seems to have a big impact, we will investigate this further by creating a scatter plot of (log-transformed) delivery time against distance.

```{r distance_log_time_traffic_level, fig.width=12, fig.height=5, message=FALSE, warning=FALSE}
# Create scatter on distance and time (log) with traffic level in colors
ggplot(df_clean, aes(x = distance_km, y = log_delivery_time, color = traffic_level_factor)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Distance vs Log Delivery Time by Traffic Level",
    x = "Distance (km)",
    y = "Log Delivery Time",
    color = "Traffic Level"
  ) +
  theme_minimal()
```

From the plot above we can see that the same delivery distances take longer under heavier traffic conditions. This is a **hint for an interaction between `distance_km` and `traffic_level_factor`**.


```{r ggpairs, fig.width=12, fig.height=5, message=FALSE, warning=FALSE}

# Create scatter plot matrix using ggpairs
selected_cols <- c("courier_age_years",
                   "courier_rating_1_to_5",
                   "temperature_celsius",
                   "humidity_percent",
                   "precipitation_mm",
                   "distance_km")

ggpairs(df_clean[, selected_cols])
```

As the above pairs plot displays no strong relationships between the continuous predictors and since all of the Pearson's correlation coefficients are low, we can conclude that no more interactions need to be included in the model.


# 3. Models

The following chapter contains all the models which we fitted for our dataset.

## 3.1 Linear Model

After investigating both numerical and categorical variables, we now begin fitting linear regression models. Our target variable is Delivery Time in minutes. For modeling purposes, we apply a log-transformation to the target variable (see EDA section for justification).

Before fitting the first model, we need to split the data into training and test sets. We do this using stratified sampling based on the factor `traffic_level_factor`. This because, during EDA we observed an imbalance in the `traffic_level_factor`, and it appeared to be a strong predictor. Therefore, stratified sampling ensures that this imbalance is proportionally reflected in both the training and test datasets.

```{r}
set.seed(42) # starting point for pseudo randomness
train_indices <- createDataPartition(df_clean$traffic_level_factor, p = 0.8, list = FALSE) # use: createDataPartition by traffic_level_factor

train_data <- df_clean[train_indices, ]
test_data  <- df_clean[-train_indices, ]
```

### 3.1.1 First Linear model

We begin by fitting a first linear model including all variables that appeared relevant based on insights from the EDA.

We include the following predictors:

- `distance_km` (numeric)
- `courier_rating_1_to_5` (numeric)
- `traffic_level_factor` (categorical)
- `order_type_factor` (categorical)

We estimate the following linear regression model as in the code section below.

```{r, echo = TRUE}
# Fitting linear model with two numeric and two categorical variables
lm.delivery1 <- lm(log_delivery_time ~ distance_km + courier_rating_1_to_5 + traffic_level_factor + order_type_factor, data = train_data)
```

```{r, collapse=TRUE}
# Model 1: Coefficients
lm.delivery1
```

<br>

#### Interpretation of model coefficients `lm.delivery1`

The `Intercept` on the log-transformed scale is estimated at 2.528. Taking the exponential of 2.528 yields 12.53, which represents the expected delivery time (in minutes) when all numeric predictors are set to 0 and all categorical predictors are at their reference levels. This value has no practical interpretation but is needed for the model.

There is strong evidence that the slope of `distance_km` is not zero. Each additional kilometer increases the log-delivery time by approximately 0.0134. The exponential of 0.0134 is equal to 1.0134, which implies an increase in delivery time of **1.34% per additional kilometer**.

There is **no significant effect** of the `courier_rating_1_to_5` on the delivery time. The p-value is 0.242 (>> 0.05) and the 95% confidence interval includes 0 ([-0.0151, 0.0038]). Therefore, we will consider removing this variable in the next step.

For `traffic_level_factor` "very low"` represents the reference level. All other levels show highly significant positive coefficients, indicating longer delivery times with increasing traffic. As an example, the coefficient of traffic level "very high" is 1.219. The exponent of 1.219 is 3.38, which means that deliveries under very high traffic take on average **3.38 times longer** than under very low traffic, holding all other variables constant.

For `order_type_factor` the reference level is `"Buffet"`. All other order types are significant, though their effects are smaller compared to traffic level. As an example, the coefficient of "Drinks" is 0.214. The exponent of 0.214 is 1.239, which means that deliveries with drinks take **23.9% longer** on average. This could be due to the delicate handling required for beverages (speculative).

<br>

#### Model performance of `lm.delivery1`

The R-squared value of the model was 0.9202. This indicates that **92.02% of the variance** in the log-transformed delivery time is explained by the model.

The adjusted R² (value = 0.9201) accounts for the number of predictors and penalizes model complexity. The fact that it is almost identical to the regular R² suggests that all included predictors contribute meaningfully to the model.

The Residual Standard Error (RSE) was 0.1332 (on the log scale). This measures the average deviation of the observed log-delivery times from the fitted values. To interpret it on the original scale we exponentiate 0.1332 and receive a value of 1.142. This can be interpreted that on average the predicted delivery time differs from observed values by approximately **14.2%**.

These three metrics suggests that the model fits the data well and explains most of the variation in the response variable.

<br>

#### Diagnostic plots for `lm.delivery1`

As Diagnostic plots for `lm.delivery1`, we created a Residuals vs Fitted Values plot and a Q-Q Plot.

```{r, fig.width=12, fig.height = 6}
# diagnostic plots

# prepare data
df_plot <- data.frame(
  fitted = fitted(lm.delivery1),
  residuals = resid(lm.delivery1)
)

# Plot: residuals vs Fitted
p1 <- ggplot(df_plot, aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()

# Create the Q-Q plot using ggplot2
p2 <- ggplot(df_plot, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

p1 + p2
```

The Residuals vs Fitted Values plot shows that residuals are roughly centered around zero, and most values lie within the range of -0.4 to +0.4. We observe slight vertical striping, this is likely to be caused by the categorical variables. There is also a mild indication of heteroscedasticity (increasing spread toward higher fitted values), suggesting that the variance is not completely constant. This would technically violate the homoscedasticity assumption of linear regression, but the effect seems minor and we will tolerate it in our case.

Additionally, the Q-Q plot indicates that residuals are approximately normally distributed. There are no major deviations from the reference line, which supports the validity of the normality assumption.

In conclusion, we do not observe any severe violations of linear model assumptions. Therefore, we proceed with the current model and consider variable selection next.


### 3.1.2 Second Linear Model

To assess the impact of individual variables, we use the `drop1()` function. This allows us to test the effect of removing each predictor while keeping the others constant.

```{r, collapse=TRUE}
# using drop1() on first linear model
drop1(lm.delivery1, test = "F")
```

Based on the output of `drop1()`, as well as the earlier `summary()`, we observe that the variable `courier_rating_1_to_5` has a high p-value, meaning we have no evidence to reject the null hypothesis of the variable's coefficient being zero. Removing this variable results in only a very small increase in the residual sum of squares (RSS), from 128.16 to 128.19 also, the Akaike Information Criterion (AIC) remains unchanged.

This suggests that excluding `courier_rating_1_to_5` does not worsen the model and should simplify interpretation. We therefore fit a second model `lm.delivery2`, without `courier_rating_1_to_5`. 

```{r, collapse=TRUE}
# Simplifying the model
# Model 2: Coefficients
lm.delivery2 <- update(lm.delivery1, . ~ . - courier_rating_1_to_5)
lm.delivery2
```


A comparison of the Coefficient estimates between `lm.delivery1` and `lm.delivery2` shows that the R-squared and Adjusted R-squared remain unchanged. The model quality is therefore practically the same.

<br>

#### AIC Comparison: Model 1 vs Model 2

To compare the two models, we use the Akaike Information Criterion (AIC). Lower values indicate a better trade-off between fit and complexity.

```{r}
AIC(lm.delivery1)
AIC(lm.delivery2)
```

Finally, the AIC values of both models are nearly identical, again supporting the simplification. We conclude, that the model can be reduced without losing explanatory information by removing `courier_rating_1_to_5`.


### 3.1.3 Third Linear Model – incl. interaction

In the next step, we explore whether an interaction exists between `distance_km` and `traffic_level_factor`. We chose to do so, because the effect of distance on delivery time may be different / stronger under heavy traffic conditions. For example, slow progress over a long distance could lead to a much longer delivery times.

We therefore fit a third model including the interaction term (see code chunk below).

```{r}
log_delivery_time ~ distance_km * traffic_level_factor + order_type_factor
```

This model includes the main effects of `distance_km`, `traffic_level_factor`, and `order_type_factor` as well as all interaction terms between `distance_km` and `traffic_level_factor`.

```{r, collapse=TRUE}
# Including interaction between distance_km and traffic_level_factor
lm.interaction <- lm(log_delivery_time ~ distance_km * traffic_level_factor + order_type_factor, data = train_data)

# Model 3 (interaction): Coefficients
lm.interaction
```

Comparing the two models we observe that the residual standard error is lower than in the previous model, showing a better fit. Both R-squared and Adjusted R-squared are slightly higher. The F-statistic from `summary()` is higher for the simpler model `lm.delivery2`, but this is due to fewer parameters (F-stat compares against a null model - having no predictors).

To formally compare both models, We use both the Akaike Information Criterion (AIC) and an ANOVA test to compare the simpler model (`lm.delivery2`) with the interaction model (`lm.interaction`).

<br>

#### AIC comparison
```{r}
# Compare AIC values
AIC(lm.delivery2)
AIC(lm.interaction)
```

The AIC for `lm.interaction` is lower than for `lm.delivery2`, indicating a better fit despite the increased complexity.

<br>

#### ANOVA comparison
```{r}
# anova to compare models
anova(lm.delivery2, lm.interaction)
```
The ANOVA test shows a significant reduction in the residual sum of squares when moving from the simpler model to the interaction model. The F-statistic is high and the corresponding p-value is very small.

Both AIC and ANOVA favor the model including the interaction. We therefore select `lm.interaction` as our preferred model. This result supports the hypothesis that the effect of distance on delivery time depends on the traffic level.


### 3.1.4 Predictions

After selecting `lm.interaction` as our final model, we now evaluate its performance on the test dataset.

We start by generating predictions on the log-scale using the `predict()` function and then transform them back to the original scale (minutes) using `exp()`. After that we compare the predicted delivery times to the actual values using scatter plots and vertical error bars to visualize the prediction error.

```{r}
# Prediction, exponentation of prediction and actual delivery time from test_data
predicted_log <- predict(lm.interaction, newdata = test_data)
predicted_min <- exp(predicted_log)
actual_min <- test_data$delivery_time_min
```


```{r, fig.width=12, fig.height = 6}
# Plotting predicted vs. actual delivery time

p1 <- ggplot(test_data, aes(x = predicted_min, y = actual_min)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    title = "Predicted vs Actual Delivery Time",
    x = "Predicted Delivery Time [min]",
    y = "Actual Delivery Time [min]"
  ) +
  theme_minimal()

# Plotting prediction errors on test data

p2 <- ggplot(test_data, aes(x = predicted_min, y = actual_min)) +
  geom_point(alpha = 0.3) +
  geom_segment(aes(xend = predicted_min, yend = predicted_min), color = "blue", alpha = 0.2) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(
    title = "Prediction Errors on Test Data",
    x = "Predicted Delivery Time [min]",
    y = "Actual Delivery Time [min]"
  ) +
  theme_minimal()

p1 + p2
```

```{r}
# calculation root mean squared error and r_squared on test data
rmse <- sqrt(mean((predicted_min - actual_min)^2))
r_squared <- 1 - sum((predicted_min - actual_min)^2) / sum((actual_min - mean(actual_min))^2)
```

```{r}
# calculating rmse for only short deliveries (i.e. < 40min)
results <- data.frame(predicted_min = predicted_min, actual_min = actual_min)
short_deliveries <- results[results$actual_min < 40, ]
rmse_short <- sqrt(mean((short_deliveries$predicted_min - short_deliveries$actual_min)^2))
rmse_short
```
We observe that for deliveries under 40 minutes, the predicted values are very close to the actual ones. The model performs well in this range. For longer delivery times (above 50 minutes), the variance increases and the predictions become less precise. A few, rather extreme outliers are present for very long delivery times. Overall, the predicted vs actual relationship appears approximately linear, and the model performs robustly.

We also calculated two common evaluation metrics, the Root Mean Squared Error (RMSE) and the R-squared. The RMSE was **5.59 minutes** and the R-squared 0.883. This tells us that the average prediction error is around 5.6 minutes and that the model explains 88.3% of the variance in delivery times on unseen data. This R² is only slightly below the training R², which suggests no overfitting.

Lasty, we computed the RMSE for short deliveries (< 40 minutes) only, which resulted in a RMSE value of 3.09. This indicates that the prediction error is on average 3.1 minutes, meaning the predictions for delivery times under 40 minutes are even more exact.

## 3.2 Generalized Linear Models

### 3.2.1 Poisson Regression
After first modeling with a linear model, we decided to conduct a further analysis on delivery speed. It is important to understand which variables significantly impact delivery speed: is it the weather conditions, traffic conditions, or even the type of vehicle? The variable `average_speed_kmph` is derived by dividing ``distance_km` by `delivery_time_min`, and then converted to an integer. In this case, `average_speed_kmph` meets the requirement for a Poisson model, which assumes count data as the response variable.

#### 3.2.1.1 Modeling

Before starting the modeling, the `traffic_level_factor` variable is convert to an unordered factor, and its reference level is set to `Very Low`, so that all other traffic levels are compared against this baseline.

In the initial Poisson model, continuous variables such as `courier_age_years`, `temperature_celsius`, `humidity_percent`, `precipitation_mm`, and `distance_km`, as well as categorical variables like `traffic_level_factor`, `vehicle_type_factor`, and `weather_category`, are included in the Poisson model in the first round to examine the validity of these predictors. The goal is to evaluate the significance of these variables in explaining variations in average_speed_kmph.
The results showed in the model summary indicates that the p-values for `courier_age_years`, `precipitation_mm`, and `vehicle_type_factor` are greater than 0.05, suggesting that they do not contribute significantly to the model. Therefore, they will not be considered in the second round of modeling.


```{r, message = FALSE, warning = FALSE, collapse=TRUE}
# read data
df_clean <- read.csv("../data/cleaned_data.csv")


#  Convert to unordered factor
df_clean$traffic_level_factor <- factor(df_clean$traffic_level_factor, ordered = FALSE)
#  Set the reference level
df_clean$traffic_level_factor <- relevel(df_clean$traffic_level_factor, ref = "very low")

# regression with glm poisson model
poisson.model.test <- glm(
  average_speed_kmph ~ courier_age_years + temperature_celsius + humidity_percent + precipitation_mm +
    distance_km + traffic_level_factor + vehicle_type_factor +
    weather_category,
  data = df_clean,
  family = poisson(link="log")
)

# check model summary
summary(poisson.model.test)
```
The second attempt of the Poisson model involves a reduced set of predictors: only `temperature_celsius`, `humidity_percent`, `distance_km`, `traffic_level_factor`, and `weather_category` are included. All remaining predictors show p-values far below 0.05, which indicates all the predictors contribute to the model. 

Another important improvement lies in the ´AIC´ values: the reduced model yields a slightly lower ´AIC´, decreasing from 51224 to 51219, suggesting a better model fit with fewer variables. The Akaike Information Criterion (AIC) is a measure used to compare the quality of statistical models, particularly for models fitted to the same dataset. It helps identify the model that best balances goodness of fit and model simplicity (@wikipedia2025aic).

Additionally, the ´residual deviance´ slightly increases from 7492.1 to 7497.7. While lower deviance generally indicates a better fit (closer to the saturated model), more parameters tend to reduce residual deviance. Therefore, a small increase does not automatically justify a more complex model (@ucla_poisson_residual_deviance; @statology_residual_deviance; @stackexchange_poisson_deviance_df).

Overall, the second Poisson model is more parsimonious and performs better based on the AIC value. The estimated model,expressed in its exponentiated form, is:
 

```{r, collapse=TRUE}
# check traffic_level_factor
table(df_clean$traffic_level_factor, useNA = "always")
# Refine the Poisson regression by removing unnecessary variables.
# regression with glm poisson model
poisson.model <- glm(
  average_speed_kmph ~ temperature_celsius + humidity_percent + distance_km + traffic_level_factor +
    weather_category,
  data = df_clean,
  family = poisson(link="log")
)

# check model summary
summary(poisson.model)
```
\[
\begin{aligned}
\text{Log(average_speed_kmph)} = 2.627 &- 0.006 \cdot \text{temperature_celsius} \\
&- 0.002 \cdot \text{humidity_percent} 
+ 0.036 \cdot \text{distance_km} \\
&+ 0.162 \cdot \text{traffic_level_factorhigh}
+ 0.212 \cdot \text{traffic_level_factorlow} \\
&+ 0.275 \cdot \text{traffic_level_factormoderate}
- 0.143 \cdot \text{traffic_level_factorvery high} \\
&- 0.046 \cdot \text{weather_categoryPoorVisibility}
- 0.072 \cdot \text{weather_categoryRainy}
\end{aligned}
\]
\

#### 3.2.1.2 Interpretation of coefficients
Because the Poisson model uses a log-link function, the coefficients are interpreted in terms of their Incidence Rate Ratios (IRRs). The IRR is the exponentiated coefficient from the Poisson regression
- *Continuous Variables*

The variable ´temperature_celsius´ has an exponentiated coefficient of 0.99, which means that for every 1°C increase in temperature, the expected delivery speed decreases by approximately 1%. This suggests that higher temperatures slightly reduce speed, possibly due to fatigue or decreased performance in hot weather.

The variable ´distance_km´ has an exponentiated coefficient of 1.04. This indicates that for every 1 km increase in distance, the expected delivery speed increases by 4%. Longer delivery distances are associated with slightly higher speeds, possibly because longer routes include faster segments like main roads or highways.

The variable humidity_percent has an exponentiated coefficient of 1.00, indicating no practical effect on delivery speed.

- *Categorical Variables*

The variable traffic_level_factor is an unordered factor with ´Very Low´ as the reference category. ´traffic_level_factorLow´ has an exponentiated coefficient of 1.24, suggesting increase 24% in speed compared to ´Very Low´. ´traffic_level_factorModerate´: 1.32, 32% increase 
´traffic_level_factorHigh´: 1.18, 18% increase
´traffic_level_factorVery High´: 0.87, a 13% decrease in speed. 
These results indicate that mild to moderate traffic conditions are associated with faster delivery speeds, possibly because very low traffic occurs at off-peak hours or in low-demand areas. However, "Very High" traffic reduces speed significantly, as expected.

For the ´weather_category´, ´weather_categoryClear´ was taken as the baseline.
´weather_categoryPoor Visibility´ results exponentialed coefficient 0.96, indicating about 4% decrease in speed. ´weather_categoryRainy´ with exponentialed coefficient 0.93, suggesting 7% decrease comparing to the baseline. These results quantitatively describe how adverse weather conditions reduce delivery speed, with rain having a slightly greater negative effect than poor visibility.


```{r}
#library(kableExtra)
# Extract variables of interest
vars <- c("traffic_level_factorhigh", 
          "traffic_level_factorlow", 
          "traffic_level_factormoderate", 
          "traffic_level_factorvery high",
          "weather_categoryPoor Visibility", 
          "weather_categoryRainy",
          "temperature_celsius", 
          "humidity_percent",
          "distance_km")

# Get coefficients and IRRs
coefs <- coef(poisson.model)[vars]
irr <- exp(coefs)

# Create data frame
coef_table <- data.frame(
  Coefficient = round(coefs, 3),
  IRR = round(irr, 2)
)

# Display as a kable table
kable(coef_table, caption = "Poisson Regression Coefficients and IRRs") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(2, width ="4cm") %>% 
  column_spec(3, width ="4cm")

```



#### 3.2.1.3 Comparison of Real vs. Simulated Delivery Speeds by Traffic Level

To assess the adequacy of the Poisson regression model, we simulate delivery speeds using the fitted model and compare them to the actual observed values. Specifically, predicted values of average_speed_kmph are generated via the simulate() function based on the fitted Poisson model. A combined dataset is then constructed that includes both the real and simulated values, with traffic_level_factor as the grouping variable.

The resulting boxplot (Figure X) shows the distribution of delivery speeds for each traffic level, separated by Real (observed) and Simulated (model-generated) speeds.

Overall, the model captures the general trend in delivery speeds across traffic conditions:

Speeds increase from "Very Low" to "Moderate" traffic levels in both real and simulated data, aligning with the previously observed coefficients.

The Simulated values closely follow the center and spread of the Real data, especially under low to moderate traffic conditions.

Slight discrepancies appear under "Very High" traffic, where the model tends to slightly overestimate speed compared to the observed values, reflecting room for model refinement in extreme conditions.

This comparison validates that the Poisson model reasonably replicates key patterns in the data and supports its use for simulating delivery behavior under varying traffic levels.

```{r, fig.align='center'}
# simulate some data from this model
set.seed(2)
sim.data.average_speed_kmph.Poisson <- simulate(poisson.model)
names(sim.data.average_speed_kmph.Poisson) <- "sim_average_speed_kmph"

# Create combined data frame for plotting
df_plot_combined <- bind_rows(
  df_clean %>%
    select(Traffic_Level = traffic_level_factor, speed = average_speed_kmph) %>%
    mutate(Type = "Real"),
  
  data.frame(
    Traffic_Level = df_clean$traffic_level_factor,
    speed = sim.data.average_speed_kmph.Poisson$sim_average_speed_kmph,
    Type = "Simulated"
  )
)

# ensures the plot uses the correct order
df_plot_combined$Traffic_Level <- factor(
  df_plot_combined$Traffic_Level,
  levels = c("very low", "low", "moderate", "high", "very high"),
  ordered = TRUE
)

# Plot grouped boxplot
ggplot(df_plot_combined, aes(x = Traffic_Level, y = speed, fill = Type)) +
  geom_boxplot(outlier.shape = 21, alpha = 0.7, position = position_dodge(width = 0.8)) +
  scale_fill_manual(values = c("Real" = "skyblue", "Simulated" = "lightpink")) +
  labs(
    title = "Figure x: Real vs. Simulated Delivery Speeds by Traffic Level",
    x = "Traffic Level",
    y = "Average Speed (km/h)",
    fill = "Speed Type"
  ) +
  theme_minimal()
```

#### 3.2.1.4 Residual Analysis

To evaluate the adequacy of the Poisson regression model, we conducted a residual analysis using deviance, Pearson, and raw residuals.

**Predicted vs. Deviance Residuals**
We visualized deviance residuals against the predicted delivery speeds to assess model fit. The residuals appear randomly scattered around zero, with no strong systematic patterns, indicating that the model captures the main structure in the data adequately. A slight fan shape at higher predicted values could hint at heteroskedasticity, but the effect is minor.

**Distribution of Deviance Residuals**
We examined the distribution of deviance residuals using a histogram. The distribution is approximately symmetric and centered around zero, which supports the assumption that the model residuals are unbiased and randomly distributed.

While the shape is roughly bell-shaped, there are a few mild outliers (e.g., residuals beyond ±5), but their frequency is very low. This suggests that the Poisson model fits the majority of observations well, no severe skewness or kurtosis is evident in the residuals, outlier influence is limited, and the model is robust to most individual observations.

**Overdispersion Check**
To check for overdispersion, we calculated the overdispersion statistic using Pearson residuals. The resulting overdispersion ratio is 0.83. Since this value is less than 1, it suggests underdispersion — meaning the observed variance in the data is slightly lower than what the Poisson distribution assumes. This is not problematic and indicates that the model may be slightly conservative in its variance estimation but overall remains valid.

**Conclusion**
The residual diagnostics show no major issues with model specification. The model appears to fit the data reasonably well, with no evidence of significant overdispersion or major misspecification.
```{r, fig.width=12, fig.height = 6}

# residual analysis
# 1.compute residuals
df_clean$residuals_raw <- residuals(poisson.model, type = "response")   # raw residuals
df_clean$residuals_deviance <- residuals(poisson.model, type = "deviance")  # deviance residuals
df_clean$residuals_pearson <- residuals(poisson.model, type = "pearson")  # for overdispersion check
df_clean$predicted_speed_kmph <- predict(poisson.model, type = "response") # Compute predicted values

# 2. Plot: Predicted vs. Residuals
p1 <- ggplot(df_clean, aes(x = predicted_speed_kmph, y = residuals_deviance)) +
  geom_point(alpha = 0.4, color = "darkorange") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Deviance Residuals vs. Predicted Speed",
    x = "Predicted Speed (km/h)",
    y = "Deviance Residuals"
  ) +
  theme_minimal()

# 3. Histogram of Residuals
p2 <- ggplot(df_clean, aes(x = residuals_deviance)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "black") +
  labs(
    title = "Histogram of Deviance Residuals",
    x = "Deviance Residuals",
    y = "Count"
  ) +
  theme_minimal()

p1 + p2

# 4. Check for Overdispersion
overdispersion <- sum(residuals(poisson.model, type = "pearson")^2) / poisson.model$df.residual
# Create a summary table
overdisp_table <- data.frame(
  Metric = "Overdispersion",
  Value = round(overdispersion, 3),
  Interpretation = if (overdispersion > 1) {
    "Overdispersion detected (> 1); consider using quasi-Poisson or negative binomial."
  } else {
    "No overdispersion (< 1); Poisson model is likely appropriate."
  }
)

# Print as a styled table
kable(overdisp_table, caption = "Overdispersion Check Result") %>%
  kable_styling(full_width = FALSE, position = "left") %>%
  column_spec(1, bold = TRUE)
```
### 3.2.2 Binomial Regression

<br>

#### Preparation

First step is preprocessing the data further for the binomial model. This includes removing the "bicycle" observations from `vehicle_type_factor` as they are too scarce and could negatively impact on model fitting.

```{r}
#  Convert to unordered factor
df_clean$weather_category <- factor(df_clean$weather_category, ordered = FALSE)
#  Set the reference level
df_clean$weather_category <- relevel(df_clean$weather_category, ref = "Clear")

# Pre-process vehicle_type_factor
df_clean_binomial <- df_clean %>%
  # Remove bicycle as only 10 observations
  filter(vehicle_type_factor != "bicycle")
```

Next, the data set is split into 80% training and 20% testing parts.  

``` {r}
# Split data into 80% train and 20% test
set.seed(42) # starting point for pseudo randomness
train_indices <- createDataPartition(df_clean_binomial$long_delivery_flag, p = 0.8, list = FALSE)
train_data <- df_clean_binomial[train_indices, ]
test_data  <- df_clean_binomial[-train_indices, ]
```

<br>

#### Modelling

As a first attempt all features are included and no interactions are modeled.

``` {r, collapse=TRUE}
# GLM Binomial model (all features)
binomial.model.test <- glm(
  long_delivery_flag ~ courier_age_years + courier_rating_1_to_5 + 
    temperature_celsius + humidity_percent +
    distance_km + order_type_factor + vehicle_type_factor +
    traffic_level_factor + weather_category,
  data = train_data,
  family = binomial
)
summary(binomial.model.test)
```

The standard errors for the parameter estimates of `traffic_level_factor` are way too large. This usually indicates a convergence issue or some degree of data separation. In addition, when fitting the logistic model with this feature, R throws the following error:
`"Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred"`
It turns out that certain classes of `traffic_level_factor` perfectly estimate `long_flag_duration`. This may be seen in the following table.

```{r}
# Complete separation occurs with traffic_level_factor
table(df_clean_binomial$long_delivery_flag, df_clean_binomial$traffic_level_factor)
```

This is problematic as it means that the maximum likelihood estimate (MLE) for the coefficients corresponding to the factor levels of `traffic_level_factor` does not exist and the model fails to converge. As a result, the interpretation of coefficients becomes unstable and meaningless. I decided to progress with a more robust solution: Firth’s correction (bias-reduced logistic regression) implemented with the `brglm2::brglmFit` method.

```{r, collapse=TRUE}
binomial.model.bias.reduced <- glm(
  long_delivery_flag ~ courier_age_years + courier_rating_1_to_5 + 
    temperature_celsius + humidity_percent +
    distance_km + order_type_factor + vehicle_type_factor +
    traffic_level_factor + weather_category,
  data = train_data,
  family = binomial,
  method = "brglmFit"
)
summary(binomial.model.bias.reduced)
```

This method resolved the data separation issue (low standard errors) and decreased the number of Fisher Scoring iterations from 20 to 7. We can see that all features except `courier_age_years`, `courier_rating_1_to_5` `humidity_percent` and `vehicle_type_factor` have a significant p-value. These features are removed in the final model.

```{r, collapse=TRUE}
binomial.model.final <- glm(
  long_delivery_flag ~ temperature_celsius +
    distance_km + order_type_factor +
    traffic_level_factor + weather_category,
  data = train_data,
  family = binomial,
  method = "brglmFit"
)
summary(binomial.model.final)
```

The loss in accuracy of the final model is tested compared to the previous with a Likelihood Ratio Test.

``` {r, collapse=TRUE}
#Likelihood Ratio Test
anova(binomial.model.bias.reduced, binomial.model.final, test="LRT")
```

The null hypothesis in this test is that the simpler model (final model) fits the data as well as the more complex model and the alternative hypothesis that the more complex model provides a significantly better fit. As p-value = 0.4806 (>> 0.05), we fail to reject the null hypothesis and can conclude there was no significant loss in model fit when dropping the aforementioned features. The final model is therefore the better option considering it is more simple.

<br>

#### Evaluation

In order to test the accuracy of the model, predictions are made for the `test_data`, a confusion matrix is created and accuracy is measured.

``` {r}
# Predictions
test_data$y_pred = predict(binomial.model.final,
  test_data[c('temperature_celsius', 'humidity_percent', 'precipitation_mm', 'distance_km', 'order_type_factor',
  'traffic_level_factor', 'weather_category')], type="response")

# Convert to class labels (using 0.5 cutoff)
test_data <- test_data %>%
  mutate(y_pred = if_else(y_pred > 0.5, 1, 0))

# Confusion matrix
table(Predicted = test_data$y_pred, Actual = test_data$long_delivery_flag)

# Accuracy
mean(test_data$y_pred == test_data$long_delivery_flag)
```

The accuracy of the model (with threshold = 0.5) is 93.2% which is very good. 54 deliveries that were shorter than 40 minutes were misclassified as longer than and 69 of deliveries that were longer than 40 minutes were misclassified as shorter than. This corresponds to a specificity of 94.9% and sensitivity of 90.7%. Furthermore, when the model predicts a short delivery, it has 93.6% probability of being correct (negative predictive value) and when it predicts a long delivery, it has a 92.6% probability of being correct.

<br>

#### Interpretation

The estimated model,expressed in its exponentiated form, is:

\[
\begin{aligned}
\text{long_delivery_flag} = \exp(&\, -16.381 
+ 0.084 \cdot \text{temperature_celsius} \\
&+ 0.243 \cdot \text{distance_km} 
+ 7.514 \cdot \text{order_type_factorDrinks} \\
&- 0.719 \cdot \text{order_type_factorMeal}
+ 0.711 \cdot \text{order_type_factorSnack} \\
&- 1.732 \cdot \text{traffic_level_factorlow}
+ 1.792 \cdot \text{traffic_level_factormoderate} \\
&+ 10.219 \cdot \text{traffic_level_factorhigh}
+ 16.605 \cdot \text{traffic_level_factorveryhigh} \\
&+ 0.448 \cdot \text{weather_categoryPoorVisibility}
+ 1.105 \cdot \text{weather_categoryRainy})
\end{aligned}
\]
\

As for the interpretation of these coefficients:

* At reference level: (temperature_celsius = 0, distance_km = 0, order_type_factor = "Buffet", traffic_level_factor = "very low", weather_category = "Clear") the probability of a long delivery is:
\[
P = \frac{e^{-16.4}}{1 + e^{-16.4}} \approx 0
\]

* For each 10 degree increase in temperature, the odds increase by:
\[
P = e^{10 * 0.08359} \approx 2 \text{ times}
\]

* Similarly, for each 10 kilometres increase in distance, the odds increase by approx. 11 times.

* Similarly for a drink instead, the odds increase by:
\[
P = e^{7.51433} \approx 1,834 \text{ times}
\]

* Similarly for a meal instead, the odds decrease by approx. 51%.

* Similarly for a snack instead, the odds increase by approx. 2 times.

* Similarly for low traffic instead, the odds decrease by approx. 82%.

* Similarly for moderate traffic instead, the odds increase by approx. 6 times.

* Similarly for high traffic instead, the odds increase by approx. 27,421 times.

* Similarly for very high traffic instead, the odds increase by approx. 16,278,568 times.

* Similarly for poor visibility instead, the odds increase by approx. 56%.

* Similarly for rainy weather instead, the odds increase by approx. 3 times.

## 3.3. Generalised Additive Models (GAM) 

After analyzing the linear model and the generalized linear model (GLM), we will now explore a more flexible and potentially more powerful approach: the Generalized Additive Model (GAM). Our goal is to improve the prediction of the target variable — `delivery_time_min`. As before, we will use the `df_clean.csv` dataset, which has been cleaned and preprocessed from the raw data. The `gam()` function is found in the `{mgcv}` package. 

### 3.3.1 Modeling

For the GAM modeling, several options can be incorporated into the model: the smoother `s()` function can be applied to continuous variables, and interactions between variables can also be considered. In the linear model, the target variable `delivery_time_min` was log-transformed. Therefore, in the GAM models, both the original and log-transformed versions of the target variable will be compared in terms of their outcomes. Due to space limitations in this document, only three selected models are presented in this chapter, and their results are compared.

**Gam model 1**

First, all continuous and categorical variables are included in an initial GAM to examine their p-values and assess which variables significantly contribute to the model and which do not. The model summary is obtained using `summary(gam.model.test1)`. Since no smooth functions s() included in `gam.model.test1`, it's equivalent to a linear model. 

The first round of results shows that the adjusted R-squared is 0.858. The statistically significant predictors, marked with '***', include the 'Intercept', 'temperature_celsius', 'humidity_percent', 'distance_km', and 'traffic_level_factor', suggesting they have a meaningful impact on delivery time. In contrast, variables such as 'courier_age_years', 'precipitation_mm', 'vehicle_type_factor', and 'weather_category' have p-values greater than 0.05 and therefore do not significantly contribute to the model. These non-significant variables will be excluded in the next modeling steps. 

```{r, warning=FALSE, message=FALSE, collapse=TRUE}
# again loading the cleaned data as df_clean
df_clean <- read.csv("../data/cleaned_data.csv")

# 1st trial gam modeling with all possible variables
gam.model.test1 <- gam(delivery_time_min ~ courier_age_years + temperature_celsius + humidity_percent 
                       + precipitation_mm + distance_km + traffic_level_factor 
                       + vehicle_type_factor + weather_category,
          data = df_clean)
summary(gam.model.test1)
```
**Gam model 2, integrate smoother and interactions**

In the second attempt, we include only the statistically significant variables identified previously. Additionally, we apply the `s()` function to all continuous variables — `temperature_celsius`, `humidity_percent`, and `distance_km` — to allow for flexible, non-linear relationships using smooth terms. To avoid overfitting, we use `bs = "cs"` (cubic regression splines with shrinkage), and set `k = 5` to limit the maximum allowable complexity for each smooth term.

Comparing the summary outputs of the two models, the GCV (Generalized Cross-Validation score) decreased from 39.086 to 33.274. GCV is especially useful for GAMs, as it directly relates to prediction error. A lower GCV indicates a better balance between goodness-of-fit and smoothness penalty (@clark_gam, @statisticsglobe_gcv, @crossvalidated_gcv). Additionally, the adjusted R-squared increased from 0.858 to 0.879, further indicating improved model performance.

The anova() function was used to compare the two models. This test assesses whether the more flexible model (Model 2) offers a statistically significant improvement in fit over the simpler one (Model 1). The F-statistic is large (397.3) and the p-value is extremely small (p < 2.2e-16), indicating that the inclusion of smooth functions in Model 2 significantly improves the fit, even though fewer predictors are included. The Df Deviance value, which represents the difference in residual deviance between the two models, is 52,579, again suggesting a better fit.

Lastly, comparing the AIC (Akaike Information Criterion), the value decreased from 58,762 (Model 1) to 57,308 (Model 2). Since lower AIC values indicate a better trade-off between model fit and complexity, this further supports Model 2 as the preferred model.

```{r, collapse=TRUE}
# 2nd Attempt: Model with Selected Variables and Smooth Terms
gam.model.test2 <- gam(delivery_time_min ~ s(temperature_celsius, bs = "cs", k=5) + 
                         s(humidity_percent, bs = "cs", k=5) + 
                         s(distance_km, bs = "cs", k=5) + 
                         traffic_level_factor + 
                         weather_category,
                       data = df_clean) 
# print the summary
summary(gam.model.test2)

# Run the anova model comparison
anova_results <- anova(gam.model.test1, gam.model.test2, test = "F")

# Convert to data frame
anova_df <- as.data.frame(anova_results)

# Create a data frame from the ANOVA results
anova_table <- data.frame(
  Model = c("gam.model.test1", "gam.model.test2"),
  Resid_Df = anova_df[["Resid. Df"]],
  Resid_Dev = round(anova_df[["Resid. Dev"]], 2),
  Df = c(NA, round(anova_df[["Df"]][2], 2)),
  Deviance = c(NA, round(anova_df[["Deviance"]][2], 2)),
  F = c(NA, round(anova_df[["F"]][2], 1)),
  Pr_F = c(NA, format.pval(anova_df[["Pr(>F)"]][2], digits = 2))
)

# Print the ANOVA results table using kable
knitr::kable(anova_table, digits = 2, align = 'c', caption = "ANOVA Comparison Between gam.model.test1 and gam.model.test2")

# run AIC comparing test1 and test2

aic_results <- AIC(gam.model.test1, gam.model.test2)

# Create data frame
model_comparison <- data.frame(
  MODEL = c("gam.model.test1", "gam.model.test2"),
  Degrees_of_Freedom = c(16.00, 19.76),
  AIC = round(aic_results$AIC, 2),
  #AIC = c(58762.26, 57307.94),
  GCV = c(39.086, 33.274),
  R2 = c(0.858, 0.879)
)

# Print the table
kable(model_comparison, digits = 2, align = 'c', caption = "Model Comparison: gam.model.test1 vs gam.model.test2")
```

```{r}
# Convert to data frame
anova_df <- as.data.frame(anova_results)

# Check column names
colnames(anova_df)
```


**Gam model 3, integrate interactions**

In the next step, interactions between predictors will be analyzed and selected for modeling. After testing several trial models, we compare two interaction candidates to evaluate their improvement over the existing model, `gam.model.test2`. The interactions `temperature_celsius:humidity_percent` and `distance_km:traffic_level_factor` are included in `gam.model.test6`, while only `temperature_celsius:humidity_percent` is included in `gam.model.test8`. Their performance is compared using the same criteria: anova(), AIC, and GCV values.

Two generalized additive models (GAMs) were compared to assess the effect of including interaction terms. `gam.model.test6` includes both interactions `temperature_celsius: humidity_percent` and `distance_km:traffic_level_factor`, while `gam.model.test8` includes only the first interaction.

`gam.model.test6` has a slightly lower AIC (57247.91 vs. 57264.50) and marginally better GCV (33.05 vs. 33.12), indicating a minor improvement in model fit. Both models achieve the same R² value of 0.88, suggesting equal explanatory power.

The interaction `temperature_celsius:humidity_percent` is statistically significant (p < 0.001) and suggests that higher temperature combined with higher humidity slightly decreases delivery time.

All interaction terms involving `distance_km:traffic_level_factor` in `gam.model.test6` are not statistically significant (p > 0.23), implying they do not contribute meaningful information.

Smooth term plots are consistent across models: temperature and humidity show modest nonlinear effects, while distance has a stronger positive effect in `gam.model.test8`.

Although `gam.model.test6` fits slightly better, the improvement is minimal, and the added complexity from the insignificant distance_km:traffic_level_factor interaction is not justified.

Therefore, `gam.model.test8` is preferable for its simplicity and comparable performance. It retains the significant interaction and avoids unnecessary terms that may lead to overfitting.

```{r, collapse=TRUE, out.width = "100%", fig.height = 2}
# 6th fitting, include recommended interations to the GAM with shrinkage of cyclic spline
gam.model.test6 <- gam(delivery_time_min ~ s(temperature_celsius, bs = "cs", k=5) + 
                                           s(humidity_percent, bs = "cs", k=5) + 
                                           s(distance_km, bs = "cs", k=5) + 
                                           traffic_level_factor + 
                                           weather_category +
                                           temperature_celsius:humidity_percent +
                                           distance_km:traffic_level_factor,
                                           data = df_clean)
summary(gam.model.test6)
#  visualize the smooth terms, Plot all smooth terms with confidence intervals 
# Set layout to 1 row, 3 columns
par(mfrow = c(1, 3))

# Plot each smooth term individually
plot(gam.model.test6, select = 1, se = TRUE, rug = TRUE, main = "Test6: Temp Smooth Term")
plot(gam.model.test6, select = 2, se = TRUE, rug = TRUE, main = "Test6: Humidity Smooth Term")
plot(gam.model.test6, select = 3, se = TRUE, rug = TRUE, main = "Test6: Distance Smooth Term")

#plot(gam.model.test6, pages = 1, se = TRUE, rug = TRUE, main="Smooth Terms of GAM Model Test 6")

# 8th continue after 6th, remove insignificant variables
gam.model.test8 <- gam(delivery_time_min ~ s(temperature_celsius, bs = "cs", k=5) + 
                         s(humidity_percent, bs = "cs", k=5) + 
                         s(distance_km, bs = "cs", k=5) + 
                         traffic_level_factor + 
                         weather_category +
                         temperature_celsius:humidity_percent,
                       data = df_clean)
summary(gam.model.test8)
#  visualize the smooth terms, Plot all smooth terms with confidence intervals 
#plot(gam.model.test8, pages = 1, se = TRUE, rug = TRUE, main="Smooth Terms of GAM Model Test 8")


# Plot each smooth term individually
plot(gam.model.test8, select = 1, se = TRUE, rug = TRUE, main = "Test8: Temp Smooth Term")
plot(gam.model.test8, select = 2, se = TRUE, rug = TRUE, main = "Test8: Humidity Smooth Term")
plot(gam.model.test8, select = 3, se = TRUE, rug = TRUE, main = "Test8: Distance Smooth Term")

# model comparing test1 and test2
anova(gam.model.test6, gam.model.test8, test = "F")
AIC(gam.model.test6, gam.model.test8)


# Create data frame
model_comparison <- data.frame(
  MODEL = c("gam.model.test6", "gam.model.test8"),
  AIC = c(57247.91, 57264.50),
  GCV = c(33.054, 33.115),
  R2 = c(0.88, 0.879)
)

# Print the table
kable(model_comparison, digits = 2, align = 'c', caption = "Model Comparison: gam.model.test6 vs gam.model.test8")
```

**Gam model 4, integrate log transformation on target variable**
In the linear model, the target variable delivery_time_min was log-transformed to improve model performance (see Section 3.1). We apply the same transformation to gam.model.test8, resulting in gam.model.logtest8, and compare the performance to the untransformed version.

The model is fitted using the log-transformed delivery time (log_delivery_time) as the response variable while retaining the same predictors and interaction term (temperature_celsius:humidity_percent).

The model achieves a high adjusted R² of 0.886, indicating that 88.6% of the variance in log-transformed delivery time is explained by the model. The GCV score is 0.0255, showing a low level of prediction error, which suggests `gam.model.logtest8` is a robust choice for modeling delivery time.

```{r, collapse=TRUE}
# Adding log transformed delivery time to d.food_time
df_clean <- df_clean %>%
  mutate(log_delivery_time = log(delivery_time_min))
# 8th continue after 6th, remove insignificant variables
gam.model.logtest8 <- gam(log_delivery_time ~       s(temperature_celsius, bs = "cs", k=5) + 
                         s(humidity_percent, bs = "cs", k=5) + 
                         s(distance_km, bs = "cs", k=5) + 
                         traffic_level_factor + 
                         weather_category +
                         temperature_celsius:humidity_percent,
                       data = df_clean)
summary(gam.model.logtest8)
```


### 3.3.2 Model Comparison and Evaluation

To evaluate the performance of different modeling approaches for predicting delivery time, three models were compared using RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) on a held-out test set:

-- Linear Model (`gam.model.test1`) – A simple GAM without smoothing terms, treating all predictors as linear.

-- GAM on Original Scale (`gam.model.test8`) – A flexible GAM using cubic regression splines (bs = "cs") for numeric predictors and including an interaction between temperature and humidity.

-- GAM with Log-Transformed Target (`gam.model.logtest8`) – Same as test8, but applied to a log-transformed delivery time to account for potential skewness and stabilize variance.

The results show:

The log-transformed GAM model achieved the lowest RMSE (5.69) and lowest MAE (4.24), indicating the best predictive accuracy overall. The GAM on the original scale (test8) also performed better than the linear model, suggesting that including non-linear effects improves model fit. The linear model (test1) showed the highest error rates, confirming that linear assumptions are too restrictive for this problem.

Overall, the results suggest that: Applying a log transformation to the target variable can further improve performance, likely due to reducing the influence of outliers and heteroscedasticity. These findings support the use of flexible models such as GAMs, especially when paired with appropriate transformations of the response variable.

```{r}
#  ---- For linear model test  ----
set.seed(99)
train_index <- createDataPartition(df_clean$delivery_time_min, p = 0.8, list = FALSE)
train_data <- df_clean[train_index, ]
test_data <- df_clean[-train_index, ]

gam.model.traintest1 <- gam(delivery_time_min ~ 
                              temperature_celsius + 
                              humidity_percent + 
                              distance_km + 
                              traffic_level_factor + 
                              weather_category, 
                            data = train_data,
                            method = "REML")
                              
pred_test1 <- predict(gam.model.traintest1, newdata = test_data)

RMSE_test1 <- sqrt(mean((pred_test1 - test_data$delivery_time_min)^2))
MAE_test1 <- mean(abs(pred_test1 - test_data$delivery_time_min))


# ---- For test8  ----
set.seed(100)
train_index <- createDataPartition(df_clean$delivery_time_min, p = 0.8, list = FALSE)
train_data <- df_clean[train_index, ]
test_data <- df_clean[-train_index, ]

gam.model.traintest8 <- gam(delivery_time_min ~
                              s(temperature_celsius, bs = "cs", k=5) + 
                              s(humidity_percent, bs = "cs", k=5) + 
                              s(distance_km, bs = "cs", k=5) + 
                              traffic_level_factor + 
                              weather_category +
                              temperature_celsius:humidity_percent,
                            data = train_data,
                            method = "REML")

pred_test8 <- predict(gam.model.traintest8, newdata = test_data)

RMSE_test8 <- sqrt(mean((pred_test8 - test_data$delivery_time_min)^2))
MAE_test8 <- mean(abs(pred_test8 - test_data$delivery_time_min))


# ---- For log transformation logtest8 ----
set.seed(101)
train_index <- createDataPartition(df_clean$log_delivery_time, p = 0.8, list = FALSE)
train_data <- df_clean[train_index, ]
test_data <- df_clean[-train_index, ]

gam.model.trainlogtest8 <- gam(log_delivery_time ~
                                 s(temperature_celsius, bs = "cs", k=5) + 
                                 s(humidity_percent, bs = "cs", k=5) + 
                                 s(distance_km, bs = "cs", k=5) + 
                                 traffic_level_factor + 
                                 weather_category +
                                 temperature_celsius:humidity_percent,
                               data = train_data,
                               method = "REML")

pred_logtest8 <- predict(gam.model.trainlogtest8, newdata = test_data)
pred_logtest8_back <- exp(pred_logtest8)

RMSE_logtest8 <- sqrt(mean((pred_logtest8_back - test_data$delivery_time_min)^2))
MAE_logtest8 <- mean(abs(pred_logtest8_back - test_data$delivery_time_min))


# ---- Model Comparison Table ----
data.frame(
  Model = c("Linear model (test 1)", "Original Scale (test8)", "Log-Transformed (logtest8)"),
  RMSE = c(RMSE_test1, RMSE_test8, RMSE_logtest8),
  MAE = c(MAE_test1, MAE_test8, MAE_logtest8)
)


```

**visualization**


```{r, out.width = "100%", fig.height = 2}
# Predicted values
df_clean$pred_lm      <- predict(gam.model.traintest1, newdata = df_clean)
df_clean$pred_gam     <- predict(gam.model.traintest8, newdata = df_clean)
df_clean$pred_log_gam <- exp(predict(gam.model.trainlogtest8, newdata = df_clean))

# Residuals
df_clean$resid_lm      <- df_clean$delivery_time_min - df_clean$pred_lm
df_clean$resid_gam     <- df_clean$delivery_time_min - df_clean$pred_gam
df_clean$resid_log_gam <- df_clean$delivery_time_min - df_clean$pred_log_gam

# ----- Residual Plots -----
par(mfrow = c(1, 3))

plot(df_clean$resid_lm, 
     main = "Residuals: Linear Model", 
     ylab = "Residuals", xlab = "Index", col = "darkgreen", pch = 20)

plot(df_clean$resid_gam, 
     main = "Residuals: Smooth GAM", 
     ylab = "Residuals", xlab = "Index", col = "blue", pch = 20)

plot(df_clean$resid_log_gam, 
     main = "Residuals: Log-Transformed GAM", 
     ylab = "Residuals", xlab = "Index", col = "purple", pch = 20)


# ----- Predicted vs Actual -----
par(mfrow = c(1, 3))

plot(df_clean$delivery_time_min, df_clean$pred_lm,
     main = "Linear Model",
     xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0, 0.6, 0, 0.3))
abline(0, 1, col = "red", lwd = 2)

plot(df_clean$delivery_time_min, df_clean$pred_gam,
     main = "Smooth GAM (Original)",
     xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0, 0, 1, 0.3))
abline(0, 1, col = "red", lwd = 2)

plot(df_clean$delivery_time_min, df_clean$pred_log_gam,
     main = "Log-Transformed GAM",
     xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0.5, 0, 0.5, 0.3))
abline(0, 1, col = "red", lwd = 2)
```

```{r, include=FALSE}
# Reset layout
par(mfrow = c(1, 1))
```

## 3.4 Support Vector Machine

In this section, we aim to classify the **traffic level** using SVM models. The use case behind it could be, that the delivery company would like to use its data to classify the traffic level and for example sell this information about the traffic situation to a website / radio station. This use case would not completely hold in a real world situation, however it shall serve as an explanation for the choice of this classifier (and the educational purpose behind it).

### 3.4.1 Data Preparation

For the SVM model, we created a new dataset, `svm_data`, containing the selected features. Specifically, we chose `distance_km` and `average_speed_kmph` as predictors, and ensured that the target variable was converted to a factor. The data was then split into training and test sets using stratified sampling to preserve the class distribution.

We deliberately limited the model to these two predictors. First, they are both continuous and provide clear, interpretable relationships with delivery time. In contrast, exploratory plots indicated that the available categorical variables (e.g., order type) did not exhibit meaningful separation between delivery time classes. We aimed to build a simple and more robust model by limiting the features, ss SVM Models are prone to overfitting.

```{r}
# Create input data for SVM
svm_data <- df_clean %>%
  select(traffic_level_factor, distance_km, average_speed_kmph)

# Ensure target variable is a factor
svm_data$traffic_level_factor <- factor(svm_data$traffic_level_factor)

# Stratified sampling for training and test set
set.seed(123)
split_index <- createDataPartition(svm_data$traffic_level_factor, p = 0.8, list = FALSE)

train_svm <- svm_data[split_index, ]
test_svm  <- svm_data[-split_index, ]
```

### 3.4.2 Modeling: First SVM Model – Linear Kernel

We start with a basic SVM model using `distance_km` and `average_speed_kmph` and applied a linear kernel with a `cost` parameter of 10. To avoid overfitting, we scaled the input variables and use the `e1071::svm()` function. Later, we will compare this model to a radial SVM and explore parameter tuning.

```{r, collapse=TRUE}
# Train SVM with linear kernel
svm_model1 <- svm(
  traffic_level_factor ~ distance_km + average_speed_kmph,
  data = train_svm,
  method = "C-classification",
  kernel = "linear",
  cost = 10,
  scale = TRUE
)

# Predict on test set
pred_svm <- predict(svm_model1, newdata = test_svm)

# Evaluate classification performance
confusionMatrix(pred_svm, test_svm$traffic_level_factor)
```

<br>

#### Model Evaluation – Linear SVM

We evaluated the prediction accuracy using a confusion matrix. We got an accuracy of 0.845, meaning 84.5% of the test set was correctly classified. The 95% confidence interval for the accuracy lies between 82.7% and 86.1%. The model performs significantly better than random guessing or choosing the most frequent class. However, the model struggles to distinguish between `"moderate"` and `"high"` traffic levels. In total 45 `"moderate"` observations were misclassified as `"high"` and 48 `"high"` observations were misclassified as `"moderate"`. This pattern is also reflected visually in the decision boundaries (see next section).

#### Visualizing SVM Decision Boundaries (Linear Kernel)

To better understand how the SVM model separates traffic levels, we visualize the decision boundaries in the feature space defined by `distance_km` and `average_speed_kmph`.

We generate a prediction grid and overlay it with the training points to illustrate the model's classification behavior.

```{r, fig.align='center'}
# Create grid of points across the feature space
xrange <- seq(min(svm_data$distance_km), max(svm_data$distance_km), length.out = 200)
yrange <- seq(min(svm_data$average_speed_kmph), max(svm_data$average_speed_kmph), length.out = 200)
grid <- expand.grid(distance_km = xrange, average_speed_kmph = yrange)

# Predict traffic level on grid
grid$predicted <- predict(svm_model1, newdata = grid)

# Plot decision boundaries with actual training points
ggplot() +
  geom_tile(data = grid, aes(x = distance_km, y = average_speed_kmph, fill = predicted), alpha = 0.3) +
  geom_point(data = train_svm, aes(x = distance_km, y = average_speed_kmph, color = traffic_level_factor), alpha = 0.7, size = 1.2) +
  scale_fill_viridis_d(option = "plasma", name = "Predicted Class") +
  scale_color_viridis_d(option = "plasma", name = "True Class") +
  labs(
    title = "SVM Decision Boundaries (Linear Kernel)",
    x = "Distance [km]",
    y = "Average Speed [km/h]"
  ) +
  theme_minimal()
```

#### Interpretation

The decision regions show that the model draws clear boundaries between traffic levels. However, the overlap between `"moderate"` and `"high"` levels is visually noticeable — this confirms the misclassifications seen in the confusion matrix. The linear kernel creates linear boundaries, which may not fully capture the complexity of the relationships in the data. Therefore, we next try a radial kernel (nonlinear).

### 3.4.4 Modeling: Second SVM Model – Radial Kernel

We now fit a second model using a **radial basis function (RBF)** kernel. This allows for nonlinear decision boundaries, potentially improving performance in cases where classes overlap in complex ways. We use the same cost parameter as before (`cost = 10`) and set `gamma = 0.1`. Both variables are scaled.

```{r, collapse=TRUE}
# Train SVM with radial kernel
svm_model2 <- svm(
  traffic_level_factor ~ distance_km + average_speed_kmph,
  data = train_svm,
  kernel = "radial",
  cost = 10,
  gamma = 0.1,
  scale = TRUE
)

# Predict on test set
pred_rbf <- predict(svm_model2, newdata = test_svm)

# Evaluate performance
confusionMatrix(pred_rbf, test_svm$traffic_level_factor)
```

<br>

#### Model Evaluation – Radial SVM

The confusion matrix for the radial SVM shows similar performance to the linear model. The overall accuracy is comparable to the linear model (with 83.16% even a bit worse). Also a similar confusion between `"moderate"` and `"high"` levels persists. This suggests that, while the radial kernel introduces nonlinear decision boundaries, it does **not significantly improve** classification performance in this case.

<br>

#### Visualizing Decision Boundaries – Radial SVM

We now plot the decision boundaries of the **radial SVM** using the same feature grid as before. Unlike the linear SVM, the radial kernel allows for nonlinear and curved boundaries. This helps us visually assess whether the model better separates overlapping classes.

```{r, fig.align='center'}
# Create prediction grid (as before)
xrange <- seq(min(svm_data$distance_km), max(svm_data$distance_km), length.out = 200)
yrange <- seq(min(svm_data$average_speed_kmph), max(svm_data$average_speed_kmph), length.out = 200)
grid <- expand.grid(distance_km = xrange, average_speed_kmph = yrange)

# Predict class for each point using the radial SVM
grid$predicted <- predict(svm_model2, newdata = grid)

# Plot decision boundaries and training points
ggplot() +
  geom_tile(data = grid, aes(x = distance_km, y = average_speed_kmph, fill = predicted), alpha = 0.3) +
  geom_point(data = train_svm, aes(x = distance_km, y = average_speed_kmph, color = traffic_level_factor), alpha = 0.7, size = 1.2) +
  scale_fill_viridis_d(option = "plasma", name = "Predicted Class") +
  scale_color_viridis_d(option = "plasma", name = "True Class") +
  labs(
    title = "SVM Decision Boundaries (Radial Kernel)",
    x = "Distance [km]",
    y = "Average Speed [km/h]"
  ) +
  theme_minimal()
```

<br>

#### Interpretation

The decision boundaries created by the radial kernel are more flexible and curved compared to the linear model. However, there is still noticeable overlap between `"moderate"` and `"high"` traffic levels. This confirms what we observed in the confusion matrix, even with a nonlinear kernel, the model still struggles in this specific region of the feature space.

Next, we attempt to improve the model via **parameter tuning** using grid search and cross-validation.

### 3.4.5 SVM Parameter Tuning – Grid Search with Cross-Validation

To potentially improve the model performance, we now perform a **grid search** for the optimal combination of `cost` and `gamma` values. We use `e1071::tune()` with 10-fold cross-validation to evaluate combinations of `cost` [0.1, 1, 10, 100] and `gamma` [0.01, 0.05, 0.1, 0.5]. This process helps identify the parameter settings that yield the highest accuracy on the training data.

Hyperparameter tuning was done on the SVM Model with the radial kernel since more hyperparameters can be tuned. By this we expect more possibilities to find a better performing model.

```{r, cache = TRUE, eval=TRUE, collapse=TRUE}
set.seed(123)  # starting point pseudo randomness

# Grid search using cross-validation
tuned_model <- tune(
  svm,
  traffic_level_factor ~ distance_km + average_speed_kmph,
  data = train_svm,
  kernel = "radial",
  ranges = list(
    cost = c(0.1, 1, 10, 100),
    gamma = c(0.01, 0.05, 0.1, 0.5)
  )
)

# Print summary of tuning results
summary(tuned_model)
```

<br>

#### Tuning Results

The output lists all combinations of `cost` and `gamma` along with their cross-validated accuracy. The combination with the **highest accuracy** is automatically selected as the best model.

We now extract this model and evaluate it on the **test dataset**.

```{r, eval=TRUE, collapse=TRUE}
# Extract best model
best_model <- tuned_model$best.model

# Make predictions on test set
pred_tuned <- predict(best_model, newdata = test_svm)

# Confusion matrix for tuned model
confusionMatrix(pred_tuned, test_svm$traffic_level_factor)
```

<br>

#### Final Evaluation – Tuned Radial SVM

The confusion matrix for the best model shows that the **overall performance is similar** to the untuned radial SVM. We therefore conclude that there is no notable improvement through tuning. The initial radial model already performed well. Furthermore the confuction between `"moderate"` and `"high"` traffic levels remains the main source of error.

In summary, the SVM models perform well in classifying traffic levels (>80% accuracy, for both kernels), though some overlaps between classes remain difficult to resolve with only two features.


## 3.5 Neural Network

In this chapter a simple neural network is used to predict `log_delivery_time`.

The EDA points out not to use the `precipitation_mm` feature. `courier_age_years` and `courier_rating_1_to_5` are also not used as they are deemed to have poor predictive power on the delivery time. The features used in the model are: `temperature_celsius`, `humidity_percent`, `distance_km`, `order_type_factor`, `vehicle_type_factor`, `traffic_level_factor` and `weather_category`.

### Processing

```{r, neural_network_preprocessing}
#  Convert to unordered factor
df_clean$weather_category <- factor(df_clean$weather_category, ordered = FALSE)
#  Set the reference level
df_clean$weather_category <- relevel(df_clean$weather_category, ref = "Clear")

# Pre-process vehicle_type_factor
df_clean_ann <- df_clean %>%
  # Remove bicycle as only 10 observations
  filter(vehicle_type_factor != "bicycle") %>%
  mutate(vehicle_type_factor = factor(vehicle_type_factor))

# Define predictors
numeric_predictors <- c("temperature_celsius", "humidity_percent", "distance_km")
categorical_predictors <- c("order_type_factor", "vehicle_type_factor",
                            "traffic_level_factor", "weather_category")

target_variable <- "log_delivery_time"
```

Firstly, the features and target are defined.

```{r, neural_network_split_data}
# Split data into train and test sets
set.seed(42)
indices <- createDataPartition(df_clean_ann[[target_variable]], p = 0.8, list = FALSE)
train <- df_clean_ann[indices, ]
test <- df_clean_ann[-indices, ]

```

The data set is split into 80% train and 20% test for more accurate evaluation.

```{r, neural_network_scale_data}
# Min-Max scale train set numeric predictors
min_vals <- sapply(train[numeric_predictors], min)
max_vals <- sapply(train[numeric_predictors], max)
train_scaled_numeric <- as.data.frame(
  mapply(function(x, minv, maxv) (x - minv) / (maxv - minv),
         train[numeric_predictors], min_vals, max_vals)
)

# Use the same scaling for test set (based on train min/max)
test_scaled_numeric <- as.data.frame(
  mapply(function(x, minv, maxv) (x - minv) / (maxv - minv),
         test[numeric_predictors], min_vals, max_vals)
)

# Scale target feature
min_target <- sapply(train[target_variable], min)
max_target <- sapply(train[target_variable], max)
train_target_scaled <- as.data.frame(
  mapply(function(x, minv, maxv) (x - minv) / (maxv - minv),
         train[target_variable], min_target, max_target)
)

# Define formula for one-hot encoding
dummy_formula <- as.formula(paste("~", paste(categorical_predictors, collapse = " + ")))

# Fit encoder on training data
dummies_model <- dummyVars(dummy_formula, data = train)

# Apply to train and test
train_encoded_categorical <- predict(dummies_model, newdata = train)
test_encoded_categorical  <- predict(dummies_model, newdata = test)

# Rename column names to remove "." which confuses neuralnet
clean_names <- function(x) {
  gsub("[^[:alnum:]_]", "_", x)  # Replace all non-alphanumeric chars with underscore
}
colnames(train_encoded_categorical) <- clean_names(colnames(train_encoded_categorical))
colnames(test_encoded_categorical)  <- clean_names(colnames(test_encoded_categorical))

# Combine new predictors and target
processed_data <- cbind(train_scaled_numeric, train_encoded_categorical, train_target_scaled)
```
As for feature engineering, numerical predictors are min-max scaled to the range of [0,1] while the categorical features are one-hot encoded. The scaling is done as the features have different ranges and this could negatively impact on model convergence.

<br>

### Modelling

```{r, cache = TRUE, neural_network_model}
set.seed(42)
delivery_time_model = neuralnet(log_delivery_time ~ temperature_celsius + humidity_percent +
                         distance_km + order_type_factor_Buffet + order_type_factor_Drinks +
                         order_type_factor_Meal + order_type_factor_Snack +
                         vehicle_type_factor_electric_scooter + vehicle_type_factor_motorcycle + 
                         vehicle_type_factor_scooter + traffic_level_factor_very_low + 
                         traffic_level_factor_low + traffic_level_factor_moderate + 
                         traffic_level_factor_high + traffic_level_factor_very_high + 
                         weather_category_Clear + weather_category_Poor_Visibility + 
                         weather_category_Rainy,
                       data = processed_data, hidden = 3 , linear.output = TRUE)
```
The neuralnet package was used to model.

<br>

### Evaluation
```{r, neural_network_evaluation, fig.align="center"}
pred_scaled <- compute(delivery_time_model, cbind(test_scaled_numeric, test_encoded_categorical))
pred_original <- pred_scaled$net.result * (max_target - min_target) + min_target

#Plot predicted vs. actual values
plot(test[[target_variable]], pred_original, col='blue', pch=16, ylab = "Predicted log delivery time", xlab = "Actual log delivery time")
abline(0,1)

# RMSE
kable(sqrt(mean((test[[target_variable]] - pred_original)^2)))

# R-squared calculation
ss_res <- sum((test[[target_variable]] - pred_original)^2)
ss_tot <- sum((test[[target_variable]] - mean(test[[target_variable]]))^2)
r_squared <- 1 - (ss_res / ss_tot)

kable(round(r_squared, 4))
```

* The model has an RMSE of around 0.11 which translates to approx. 1-2 minutes.

* R squared is approx. 94% which is very good.

* The chart shows that predicted values have a strong relationship with actual values.

<br>

### Interpretation
```{r, neural_network_interpretation, fig.align="center"}
plot(delivery_time_model)
```

As visible in the diagram above, the architecture of the model has 3 nodes in the hidden layer. In summary, the model performs very well in predicting the log delivery time. As it takes approx. 20 minutes to fit, cross validation and network architecture tuning were not carried out.


# 4. Results and Discussion

Across all modeling approaches, we found that `distance_km` and `traffic_level_factor` were the most important predictors of delivery time, consistently contributing significant effects in linear, generalized linear, and GAM models. Log-transforming the delivery time improved model performance, especially for Generalized Additive Models.

The linear model with interaction already explained a large portion of the variance (R² ~ 88%) and achieved the lowest RMSE (5.59 minutes) on the test set, making it the best-performing model in terms of prediction accuracy for delivery times in this project. The log-transformed GAM model produced comparable results, with a slightly higher R² (0.886) but a slightly higher RMSE (5.69 minutes).

The Poisson regression successfully modeled delivery speeds and provided interpretable coefficients quantifying how traffic and weather affect courier speed. Additionally, the binomial model for classifying long deliveries showed very strong predictive power, achieving over 93% accuracy.

The SVM classifier achieved good accuracy (~84%) in predicting traffic levels based on distance and average speed. However, distinguishing between "moderate" and "high" traffic levels remained challenging.

Overall, our modeling results demonstrate that flexible approaches such as GAMs can provide valuable advantages for this type of data, especially when modeling potential nonlinear relationships and interactions — although, in this case, a well-tuned linear model with interaction terms provided the best predictive performance.

# 5. Conclusion

This project showed that machine learning models can effectively predict food delivery times by combining geographic, traffic, weather, and order-related data.

We found that simple, well-designed models — especially linear models with interaction terms — already provide strong predictive performance. Generalized Additive Models (GAMs) added flexibility for modeling nonlinear effects, which could become even more useful in the case that data complexity increases.

Distance and traffic level clearly emerged as being the most important factors influencing delivery time. The Poisson and binomial models also demonstrated how different modeling techniques can answer various business related questions, such as predicting delivery speeds or classifying long deliveries (e.g. to pick suitable packaging).

Furthermore Support Vector Machines performed reasonably well for traffic classification, they were limited by feature overlap.

In practice, models like these could help improve ETA accuracy, support better resource planning, and enhance customer experience. The interesting aspect of this group work was that in real life, we use applications that use ETA predictions (i.e. Uber Eats). It was therefore very interesting to see (in a limited and much easier setting) what is behind the ETA indication of such an app.

Future work could include adding real-time data (e.g. live traffic), looking for more features (e.g. time of the day), or testing more advanced models.

# 6 Chapter on AI 

In this project, we used AI tools (specifically ChatGPT) to support code revision and improvement during the development of our analysis.

For example we used ChatGPT for the following tasks:

  - Refining code syntax and structure (e.g. improving clarity of ggplot visualizations)

  - Debugging: when certain code chunks produced errors or warnings, we asked ChatGPT to help identify the issue and suggest corrections

  - Improving explanations: we used AI to help rewrite parts of the text (e.g. model interpretation sections) to make them clearer and more consistent

Throughout the project, we always double-checked any code or text we got from AI. We tested all suggestions first before adding them to our final version, and made sure that explanations matched our actual results.

Overall, using AI was very helpful for improving our code and the report, but it was important to review everything carefully and make sure it really fit with what we had done and understood.

# References
