---
title: "A Data-Driven Approach to Predicting Food Delivery Time: A Multi-Model Machine Learning Framework"
author: "Krzysztof Baran, Curdin Caderas, Wenxing Xu"
date: "2025-05-07"
bibliography: references.bib
csl: apa.csl
output: 
    html_document:
      code_folding: hide
    df_print: paged
knitr: 
  opts_chunk:
    warning:false
    message:false
---

# 1. Introduction

Brief introduction to the project, objectives, and relevance.

# 2. Data Description

## 2.1 Data Source

1- What is it about 

2- Where is it from 

3- What columns are in there 

4- Data Preprocessing 

## 2.2 Data Cleaning and Exploratory Data Analysis

Before starting any exploratory data analysis, we decided to modify the column names in the raw data to enhance the readability of the entire report.

```{r load_libraries, warning=FALSE, message=FALSE}
# Load required libraries
library(dplyr)
library(ggplot2)
library(ggspatial)
library(terra)
library(sf)
library(maptiles)
library(gridExtra)
library(patchwork)
library(GGally)
library(caret)
library(broom)
library(knitr)
library(e1071)
library(tidyr)
library(stringr)
library(brglm2)
library(mgcv)
#suppressPackageStartupMessages(library(tidyverse))
#library(tidyverse)


# Load the raw data
df.food_time <- read.csv("../data/Food_Time_Data_Set.csv")

# Renaming column names to snake case (incl. more descriptive names)

df.food_time <- df.food_time %>%
  rename(
    order_id = ID,
    courier_id = Delivery_person_ID,
    courier_age_years = Delivery_person_Age,
    courier_rating_1_to_5 = Delivery_person_Ratings,
    restaurant_latitude_deg = Restaurant_latitude,
    restaurant_longitude_deg = Restaurant_longitude,
    customer_latitude_deg = Delivery_location_latitude,
    customer_longitude_deg = Delivery_location_longitude,
    order_type = Type_of_order,
    vehicle_type = Type_of_vehicle,
    temperature_celsius = temperature,
    humidity_percent = humidity,
    precipitation_mm = precipitation,
    weather_description = weather_description,
    traffic_level = Traffic_Level,
    distance_km = Distance..km.,
    delivery_time_min = TARGET
  )
```


### 2.2.1 Geographic Filtering
To gain an initial understanding of the data set, we begin by examining the geographical distribution of the delivery observations. Four columns provide relevant location information:

`restaurant_longitude_deg`, `Restaurant_longitude`, `Delivery_location_latitude`, `Delivery_location_longitude`. All delivery distances are under 50 km.

Figures 1 and 2 show histograms of the restaurants' latitude and longitude. The values span a broad range: latitudes from -30 to 30 and longitudes from -80 to 80. However, the distribution is uneven, with most data points concentrated at latitudes greater than 0 and longitudes greater than 60.

To narrow the data set to the relevant geographic area, we applied a filter to retain only observations with latitude ≥ 0 and longitude ≥ 60. This refinement reduced the data set from 10,001 to 9,084 entries, all located within a single country—India.

To visualize the geographic distribution of these observations, we first defined the map boundaries using the data set’s minimum and maximum latitude and longitude values. These coordinates were converted into a simple feature object representing the corners of the bounding box. This object was then used to download map tiles corresponding to the area of interest, enabling accurate spatial visualization (Figure 3).

Figure 4 displays all restaurant and delivery locations overlaid on the background map from Figure 3. The plotted points indicate that the observations were collected from 22 cities across India, including major urban centers such as New Delhi, Mumbai, Bangalore, Kolkata, and Chennai.


```{r geographic_filtering, fig.height=4, fig.align='center', warning=FALSE, message=FALSE, exec = FALSE}
# Clean the data by filtering for geographic coordinates
df.food_time.clean <- df.food_time %>%
  filter(
    customer_longitude_deg >= 60,
    restaurant_longitude_deg >= 60,
    customer_latitude_deg >= 0,
    restaurant_latitude_deg >= 0
  )

# Create bounding box using the cleaned data
bbox.sf <- st_as_sf(
  data.frame(
    Latitude = c(min(df.food_time.clean$restaurant_latitude_deg), 
                 max(df.food_time.clean$restaurant_latitude_deg)),
    Longitude = c(min(df.food_time.clean$restaurant_longitude_deg), 
                  max(df.food_time.clean$restaurant_longitude_deg))
  ),
  coords = c("Longitude", "Latitude"),
  crs = "+proj=longlat"
)

# Download map tiles
example.map <- get_tiles(bbox.sf, provider = "OpenStreetMap")

# Extract bounding box limits
bbox.limits <- st_bbox(bbox.sf)

# Figure 4: Plot restaurant and delivery locations on the map
ggplot() +
  layer_spatial(data = example.map) +
  geom_point(data = df.food_time.clean,
             aes(x = restaurant_longitude_deg, y = restaurant_latitude_deg),
             color = "blue", size = 2, alpha = 0.7, shape = 16) +
  geom_point(data = df.food_time.clean,
             aes(x = customer_longitude_deg, y = customer_latitude_deg),
             color = "red", size = 2, alpha = 0.7, shape = 16) +
  coord_sf(xlim = bbox.limits[c(1, 3)], ylim = bbox.limits[c(2, 4)]) +
  labs(title = "Figure 1: Restaurant and Delivery Locations",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

### 2.2.2 Data Cleaning and Feature Engineering

To prepare the data set for analysis, several cleaning and transformation steps were performed:

- Conversion of Numerical Values:
The `distance_km` and `delivery_time_min` columns, originally stored as character strings with potential non-numeric characters, were cleaned using regular expressions and converted to numeric format.

- Categorical Variable Cleaning:
The `order_type` and `vehicle_type` columns were stripped of leading/trailing whitespaces and converted to factors to facilitate modeling and analysis.

- Traffic Level Standardization:
The `traffic_level` column was converted to lowercase and trimmed to remove inconsistencies, then stored as a factor variable `traffic_level_factor`.

- Weather Description Grouping:
The original `weather_description` field was cleaned and mapped into three broader weather categories:

  - `Clear`: includes terms like "clear sky" or "few clouds"

  - `Poor Visibility`: includes terms such as "fog" and "haze"

  - `Rainy`: includes various rain-related terms

These were stored as an ordered factor in a new variable:  `weather_category`.

- Long Delivery Flag:
A binary variable `long_delivery_flag` was created to indicate whether a delivery took 40 minutes or longer. This variable serves as the target for the binomial model. 

- Average Speed Calculation:
Delivery speed `average_speed_kmph` was computed in kilometers per hour by dividing distance by delivery time, and then converting to an integer. This variable serves as the target for the Poisson model. 

- Missing Data Removal:
All rows containing missing values after the above transformations were dropped to ensure data quality.

The result is a cleaned and enriched data set containing 9,035 observations, ready for exploratory data analysis and predictive modeling.


```{r data_cleaning, warning=FALSE, message=FALSE}
#This part will do the following:
#- Clean numeric values and convert to numeric
#- Clean and convert categorical variables to factors
#- Clean traffic level
#- Clean weather description & create the three categories Clear / Poor Visibility / Rainy
#- Insert a flag for long delivery time (if >= 40 min)
#- Calculate the average speed in km/h
#- Remove rows with NA values
#- Filter out coordinates which are not in India

df_clean <- df.food_time.clean %>%
  select(-X) %>%
  mutate(
    # Clean numeric values and convert to numeric
    distance_km = as.numeric(na_if(gsub("[^0-9\\.]", "", distance_km), "")),  
    delivery_time_min = as.numeric(na_if(gsub("[^0-9\\.]", "", delivery_time_min), "")),

    # Clean and convert categorical variables to factors
    order_type_factor = factor(str_trim(order_type)),
    vehicle_type_factor = factor(str_trim(vehicle_type)),

    # Clean traffic level
    traffic_level_cleaned = str_trim(tolower(traffic_level)),
    traffic_level_factor = factor(traffic_level_cleaned),

    # Clean weather description
    weather_description_cleaned = str_trim(tolower(weather_description)),
    weather_category = factor(case_when(
      weather_description_cleaned %in% c("broken clouds", "clear sky", "few clouds", "overcast clouds", "scattered clouds") ~ "Clear",
      weather_description_cleaned %in% c("fog", "haze", "smoke") ~ "Poor Visibility",
      weather_description_cleaned %in% c("mist", "moderate rain", "light rain") ~ "Rainy",
      TRUE ~ NA_character_
    ), levels = c("Clear", "Poor Visibility", "Rainy"), ordered = TRUE),

    # Flag for long delivery time
    long_delivery_flag = if_else(delivery_time_min >= 40, 1, 0),

    # Calculate average speed in km/h
    average_speed_kmph = as.integer(if_else(delivery_time_min > 0, distance_km * 60 / delivery_time_min, NA_real_))
  ) %>%
  drop_na()
```

```{r data_info, collapse=TRUE}
# Check for missing values in cleaned data & str
colSums(is.na(df_clean))
str(df_clean)
dim(df_clean)

# View the cleaned dataset
head(df_clean)
write.csv(df_clean, "../data/cleaned_data.csv", row.names = FALSE)
```

### 2.2.3 Exploratory Data Analysis

To start the EDA we will look at the distribution of our (main) target variable: `delivery_time_min`.
In order to do this we create a histogram and a boxplot. Furthermore, we look at the summary of this column. 

```{r delivery_time_eda, fig.width=12, fig.height=5, message=FALSE, warning=FALSE}
# Create histogram of target variable "Delivery Time [min]"
p1 <- ggplot(df_clean, aes(x = delivery_time_min)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
  labs(title = "Distribution of Delivery Time [min]", x = "Delivery Time (min)", y = "Count") +
  theme_minimal()

# Boxplot of target variable
p2 <- ggplot(df_clean, aes(y = delivery_time_min)) +
  geom_boxplot(fill = "skyblue") +
  labs(title = "Boxplot of Delivery Time", y = "Delivery Time (min)") +
  theme_minimal()

# Display next to each other
p1 + p2

# Summary
summary(df_clean$delivery_time_min)
```

What we can see from looking at the above plots of our target variable is that the distribution seems right-skewed. This can be seen on the histogram itself (flattening more slowly to the right) as well as in the boxplot (many outliers above the upper whisker).

Furthermore, this can be seen by looking at the summary:

- Mean > Median
- Max value is approx. four times the size of median.

In addition to our observations, we know that time is considered an "amount". During lectures we learned that it is advisable to log-transform amounts.

Following the aforementioned points, in the next step we log-transform our target variable into `log_delivery_time` and include it in the dataframe `df_cleaned`. To verify the log-transformation we will create the same plots and output as for the original target variable.

```{r log_delivery_time_eda, fig.width=12, fig.height=5, message=FALSE, warning=FALSE}
# Add log transformed delivery time to df_clean
df_clean$log_delivery_time <- log(df_clean$delivery_time_min)

# Plot Histogram of log-transformed delivery time
p3 <- ggplot(df_clean, aes(x = log_delivery_time)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  labs(title = "Histogram of Log-Transformed Delivery Time", x = "log(Delivery Time)", y = "Count") +
  theme_minimal()

# Create boxplot of log transformed delivery time
p4 <- ggplot(df_clean, aes(y = log_delivery_time)) +
  geom_boxplot(fill = "skyblue") +
  labs(title = "Boxplot of Log-Transformed Delivery Time", y = "log(Delivery Time)") +
  theme_minimal()

# Display next to each other
p3 + p4

# Summary
summary(df_clean$log_delivery_time)
```

After log-transforming our target variable, the distribution looks more symmetrical. The boxplot looks more compact and has less outliers above the upper whisker. Furthermore, we can see that mean and median are closer to each other.

Based on these observations (and the strong hint in our lecture notes), we decided to further proceed with the log-transformed target variable.

After investigating the target variable, we now check the distributions of the numeric predictors.


```{r predictor_histograms, fig.width=12, fig.height=8, message=FALSE, warning=FALSE}
# Select numeric variables for plot
numeric_vars <- df_clean %>%
  select(distance_km, courier_age_years, courier_rating_1_to_5,
         temperature_celsius, humidity_percent, precipitation_mm)

# Plot distribution of numeric variables
plots <- lapply(names(numeric_vars), function(var) {
  ggplot(df_clean, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "steelblue", color = "white") +
    labs(title = var, x = var, y = "Count") +
    theme_minimal()
})

# Arrange plots (using patchwork)
wrap_plots(plots, ncol = 3)
```

Looking at the different distributions we observe the following:

- **distance_km:** Most deliveries happen within ~ 5 - 20 km with a few extreme values beyond 40 km. This variable is right-skewed.

- **courier_age_years:** Rather uniform distribution in the range of ~20 to 40 years.

- **courier_rating_1_to_5:** Most ratings are clustered near 5. This suggests that ratings are overall high with not too much variability. It could be that there is only limited predictive power from this predictor (to be further investigated).

- **temperature_celsius:** Centered around ~22°C, symmetrical shape

- **humidity_percent:** Rather high variability. This could be due to different geographic areas.

- **precipitation_mm:** Many zero values which leads us to believe that there could be an error in the data. We will not use this predictor for further analysis.


In the next step we will investigate the numeric predictors further by looking at scatter plots of each numeric predictor on the (log-transformed) target variable.

```{r predictor_scatterplots, fig.width=12, fig.height=8, message=FALSE, warning=FALSE}

# Select numeric predictors (without precipitation_mm)
numeric_predictors <- c("distance_km", "courier_age_years", "courier_rating_1_to_5",
                        "temperature_celsius", "humidity_percent")

# Create scatter plots incl. correlation coefficient
scatter_plots <- lapply(numeric_predictors, function(var) {
  # Pearson-Korrelation berechnen (na.rm = TRUE für Sicherheit)
  cor_val <- round(cor(df_clean[[var]], df_clean$log_delivery_time, use = "complete.obs"), 2)
  
  # Create plot itself
  ggplot(df_clean, aes_string(x = var, y = "log_delivery_time")) +
    geom_point(alpha = 0.3, color = "black") +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    annotate("text", x = Inf, y = Inf, hjust = 1.1, vjust = 1.5,
             label = paste0("r = ", cor_val),
             size = 4, fontface = "italic") +
    labs(title = paste(var, "vs. Log Delivery Time"), x = var, y = "log(Delivery Time)") +
    theme_minimal()
})
# Arrange plots (using patchwork)
wrap_plots(scatter_plots, ncol = 3)
```

From looking at the plots, the correlation line and Pearson's r, we observe the following relationship with the log-transformed target variable:

- **distance_km:** seems to be a strong positive relationship (r = 0.84). i.e. the farther the delivery distance, the longer the (log-transformed) delivery time. This numeric predictor is the most important in our data set.

- **courier_age_years:** There is essentially no correlation between the courier age and (log-transformed) delivery time (r = 0.01). 

- **courier_rating_1_to_5:** there seems to be a rather weak negative relationship (r = 0.1). This could be interpreted that higher-rated couriers tend to deliver slightly faster. However the effect seems very weak and the data is clustered around high ratings which could limit the usefulness of this predictor.

- **temperature_celsius:** there seems to be a very weak positive relationship (r = 0.06), practically not meaningful.

- **humidity_percent:** There is essentially no correlation between humidity and delivery time (r = - 0.02, scattered points around flat line).

Among all numeric predictors, only `distance_km` shows a strong (positive) linear relationship with the log-transformed delivery time. All other variables exhibit weak to negligible correlations.


For out categorical predictors we start by investigating the counts in each category.
```{r predictor_bar_charts, fig.width=12, fig.height=10, message=FALSE, warning=FALSE}

# Relevel factor level in meaningful way
df_clean$traffic_level_factor <- factor(
  df_clean$traffic_level_factor,
  levels = c("very low", "low", "moderate", "high", "very high")
)

# Plot counts for categorical variables
p1 <- ggplot(df_clean, aes(x = order_type_factor, fill = order_type_factor)) +
  geom_bar() +
  labs(title = "Orders by Type", x = "Order Type", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

p2 <- ggplot(df_clean, aes(x = vehicle_type_factor, fill = vehicle_type_factor)) +
  geom_bar() +
  labs(title = "Orders by Vehicle Type", x = "Vehicle Type", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

p3 <- ggplot(df_clean, aes(x = traffic_level_factor, fill = traffic_level_factor)) +
  geom_bar() +
  labs(title = "Orders by Traffic Level", x = "Traffic Level", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

p4 <- ggplot(df_clean, aes(x = weather_category, fill = weather_category)) +
  geom_bar() +
  labs(title = "Orders by Weather", x = "Weather", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

(p1 / p2 / p3 / p4)
```

From the bar charts above, we observe the following:

- **order_type_factor:** all categories (buffer, drinks, meal, snack) are similarly frequent. There is no major imbalance. This means that our models can learn equally well across all order types.

- **vehicle_type_factor:** Motorcycles are the most preferred delivery method, followed by scooters. Electric scooters and bicycles are rare. The only few data points for non-motorized vehicles may reduce model performance, this is to be kept in mind when fitting the different models.

- **traffic_level_factor:** Most deliveries happen under moderate or high traffic. Very low traffic is the least frequent. Also here this imbalance needs to be kept in consideration when fitting the model and splitting into test / training data.

- **weather_category:** Most deliveries happen in clear weather, while poor visibility and rain occur less often. 



After looking at the counts of the categorical variables, we now look at the log-transformed delivery time versus our categorical variables using boxplots.

```{r predictor_boxplots, fig.width=12, fig.height=14, message=FALSE, warning=FALSE}

# Create boxplots on delivery time by categorical variables
b1 <- ggplot(df_clean, aes(x = order_type_factor, y = log_delivery_time, fill = order_type_factor)) +
  geom_boxplot() +
  labs(title = "Log Delivery Time by Order Type", x = "Order Type", y = "log(Delivery Time)") +
  theme_minimal() + theme(legend.position = "none")

b2 <- ggplot(df_clean, aes(x = vehicle_type_factor, y = log_delivery_time, fill = vehicle_type_factor)) +
  geom_boxplot() +
  labs(title = "Log Delivery Time by Vehicle Type", x = "Vehicle Type", y = "log(Delivery Time)") +
  theme_minimal() + theme(legend.position = "none")

b3 <- ggplot(df_clean, aes(x = traffic_level_factor, y = log_delivery_time, fill = traffic_level_factor)) +
  geom_boxplot() +
  labs(title = "Log Delivery Time by Traffic Level", x = "Traffic Level", y = "log(Delivery Time)") +
  theme_minimal() + theme(legend.position = "none")

b4 <- ggplot(df_clean, aes(x = weather_category, y = log_delivery_time, fill = weather_category)) +
  geom_boxplot() +
  labs(title = "Log Delivery Time by Weather", x = "Weather", y = "log(Delivery Time)") +
  theme_minimal() + theme(legend.position = "none")

(b1 / b2 / b3 / b4)
```

By visually inspecting the box plots we can make the following observations:

- **order_type_factor:** Order types seem not to differ too much in regards to their log-transformed delivery time. However, we can see that "drinks" take longer that the other factors. One reason could be that handling might be more difficult for drinks.

- **vehicle_type_factor:** A similar picture emerges when looking at the vehicle type. Only "bicycle" seems to deviate with having a lower median in delivery time. However, the count of bicycles in the data is very low. Moreover, these few delivery jobs where a bicycle was used were for shorter distances (not displayed in this report).

- **traffic_level_factor:** There is a clear upward trend from factor "very low" to "very high". i.e. the median of `log_delivery_time` increases with the traffic intensity. This would mean that heavier traffic leads to longer delivery times.

- **weather_category:** For the weather category "clear" the median delivery time seems to be slightly longer than for the other two categories. However, there are also many more deliveries during clear weather in our data set.

Since we have observed that traffic level seems to have a big impact, we will investigate this further by creating a scatter plot of (log-transformed) delivery time against distance.

```{r distance_log_time_traffic_level, fig.width=12, fig.height=5, message=FALSE, warning=FALSE}
# Create scatter on distance and time (log) with traffic level in colors
ggplot(df_clean, aes(x = distance_km, y = log_delivery_time, color = traffic_level_factor)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Distance vs Log Delivery Time by Traffic Level",
    x = "Distance (km)",
    y = "Log Delivery Time",
    color = "Traffic Level"
  ) +
  theme_minimal()
```

From the plot above we can see that the same delivery distances take longer under heavier traffic conditions. This is a **hint for an interaction between `distance_km` and `traffic_level_factor`**.


```{r ggpairs, fig.width=12, fig.height=5, message=FALSE, warning=FALSE}

# Create scatter plot matrix using ggpairs
selected_cols <- c("courier_age_years",
                   "courier_rating_1_to_5",
                   "temperature_celsius",
                   "humidity_percent",
                   "precipitation_mm",
                   "distance_km")

ggpairs(df_clean[, selected_cols])
```

As the above pairs plot displays no strong relationships between the continuous predictors and since all of the Pearson's correlation coefficients are low, we can conclude that no more interactions need to be included in the model.


# 3. Modelling 

The following chapter will contain all the models which we fitted for our dataset.

This includes:

- Linear Model
- Generalised Linear Model
- Generalized Additive Model
- Support Vector Machine
- Artificial neural network



## 3.1 Linear Model

After investigating both numerical and categorical variables, we now begin fitting linear regression models. Our target variable is **Delivery Time in minutes**. For modeling purposes, we apply a **log-transformation** to the target variable (see EDA section for justification).

Before fitting the first model, we need to split the data into training and test sets. We do this using **stratified sampling based on the factor `traffic_level_factor`**.

**Reasoning:**  
During EDA, we observed an imbalance in the `traffic_level_factor`, and it appeared to be a strong predictor. Therefore, stratified sampling ensures that this imbalance is proportionally reflected in both the training and test datasets.

```{r}
set.seed(42) # starting point for pseudo randomness
train_indices <- createDataPartition(df_clean$traffic_level_factor, p = 0.8, list = FALSE) # use: createDataPartition by traffic_level_factor

train_data <- df_clean[train_indices, ]
test_data  <- df_clean[-train_indices, ]
```

### 3.1.1 First Linear model - no interaction

We begin by fitting a first linear model including all variables that appeared relevant based on insights from the EDA.

We include the following predictors:

- `distance_km` (numeric)
- `courier_rating_1_to_5` (numeric)
- `traffic_level_factor` (categorical)
- `order_type_factor` (categorical)

We estimate the following linear regression model:

$$
y_i = \beta_0 
+ \beta_1 \cdot x_{1i} 
+ \beta_2 \cdot x_{2i} 
+ \sum_{k=1}^{K} \gamma_k \cdot d_{ki} 
+ \sum_{j=1}^{J} \delta_j \cdot c_{ji} 
+ \varepsilon_i
$$
**Legend:**

- \( y_i \): Response variable for observation \( i \), here: log-transformed delivery time  
- \( x_{1i}, x_{2i} \): Numeric predictor variables (here: delivery distance, courier rating)  
- \( d_{ki} \): Dummy variables for the first categorical predictor (e.g., traffic level), where \( k = 1, \dots, K \)  
- \( c_{ji} \): Dummy variables for the second categorical predictor (e.g., order type), where \( j = 1, \dots, J \)  
- \( \beta_0 \): Intercept term  
- \( \beta_1, \beta_2 \): Coefficients for numeric predictors  
- \( \gamma_k \): Coefficients for dummy variables of the first categorical predictor  
- \( \delta_j \): Coefficients for dummy variables of the second categorical predictor  
- \( \varepsilon_i \): Error term capturing the residual for observation \( i \)

*Note: For each categorical variable, one level is omitted (reference category), and the others are represented through dummy variables.*

```{r, echo = TRUE}
# Fitting linear model with two numeric and two categorical variables
lm.delivery1 <- lm(log_delivery_time ~ distance_km + courier_rating_1_to_5 + traffic_level_factor + order_type_factor, data = train_data)
```

```{r, collapse=TRUE}
# Model 1: Coefficients
lm.delivery1
```

#### Interpretation of model `lm.delivery1`

**`Intercept`:** 

The intercept on the log-transformed scale is estimated at 2.528.  
\[
\exp(2.528) \approx 12.53
\]  
This corresponds to an expected delivery time of around 12.53 minutes when all numeric predictors are 0 and all categorical variables are at their reference levels. This value has no practical interpretation but is needed for the model.

**`distance_km`:**  

There is strong evidence that the slope of `distance_km` is not zero. Each additional kilometer increases the log-delivery time by approximately 0.0134.  
On the original scale:
\[
\exp(0.0134) \approx 1.0134
\]  
This implies an increase in delivery time of **1.34% per additional kilometer**.

**`courier_rating_1_to_5`:**  

There is **no significant effect** of the courier rating on the delivery time.  
- The p-value is 0.242 (>> 0.05).  
- The 95% confidence interval includes 0: \([-0.0151,\ 0.0038]\)

Therefore, we will consider removing this variable in the next step.

**`traffic_level_factor`:**  

"very low"` represents the reference level. All other levels show highly significant positive coefficients, indicating longer delivery times with increasing traffic.

Example – `"very high"` traffic:  
Coefficient: 1.219  
\[
\exp(1.219) \approx 3.38
\]  
This means that deliveries under very high traffic take on average **3.38 times longer** than under very low traffic, holding all other variables constant.

**`order_type_factor`:**  

Reference level is `"Buffet"`. All other order types are significant, though their effects are smaller compared to traffic level.

Example – `"Drinks"`:  
Coefficient: 0.214  
\[
\exp(0.214) \approx 1.239
\]  
Deliveries with drinks take **23.9% longer** on average. This could be due to the delicate handling required for beverages (speculative).



#### Model performance of `lm.delivery1`

**R-squared: 0.9202**  

This indicates that **92.02% of the variance** in the log-transformed delivery time is explained by the model.

**Adjusted R-squared: 0.9201**  

Adjusted R² accounts for the number of predictors and penalizes model complexity. The fact that it is almost identical to the regular R² suggests that all included predictors contribute meaningfully to the model.

**Residual Standard Error (RSE): 0.1332 (on log scale)**  

This measures the average deviation of the observed log-delivery times from the fitted values. To interpret it on the original scale:

\[
\exp(0.1332) \approx 1.142
\]

On average, predicted delivery times differ from observed values by approximately **14.2%**.

Overall, this suggests that the model fits the data very well and explains most of the variation in the response variable.


#### Diagnostic plots for `lm.delivery1`

We created two diagnostic plots to assess the quality of our model:

```{r, fig.width=12, fig.height = 6}
# diagnostic plots

# prepare data
df_plot <- data.frame(
  fitted = fitted(lm.delivery1),
  residuals = resid(lm.delivery1)
)

# Plot: residuals vs Fitted
p1 <- ggplot(df_plot, aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()

# Create the Q-Q plot using ggplot2
p2 <- ggplot(df_plot, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

p1 + p2
```

1. **Residuals vs Fitted Values**  
This plot shows that residuals are roughly centered around zero, and most values lie within the range of -0.4 to +0.4.  
We observe slight vertical striping, this is likely to be caused by the categorical variables.  
There is also a mild indication of **heteroscedasticity** (increasing spread toward higher fitted values), suggesting that the variance is not completely constant.  
This would technically violate the homoscedasticity assumption of linear regression, but the effect seems minor and we will tolerate it in our case.

2. **Q-Q Plot**  
The Q-Q plot indicates that residuals are approximately normally distributed.  
There are no major deviations from the reference line, which supports the validity of the normality assumption.

Conclusion:  
We do not observe any severe violations of linear model assumptions. Therefore, we proceed with the current model and consider variable selection next.


### 3.1.2 Second Linear Model – simplification of model 1

To assess the impact of individual variables, we use the `drop1()` function. This allows us to test the effect of removing each predictor while keeping the others constant.

```{r, collapse=TRUE}
# using drop1() on first linear model
drop1(lm.delivery1, test = "F")
```

Based on the output of `drop1()`, as well as the earlier `summary()`, we observe the following:

- The variable `courier_rating_1_to_5` has a **high p-value**, indicating no significant contribution to the model.
- Removing this variable results in only a **very small increase** in the residual sum of squares (RSS), from 128.16 to 128.19.
- The **Akaike Information Criterion (AIC)** remains unchanged.

This suggests that excluding `courier_rating_1_to_5` does not worsen the model and should simplify interpretation.

We therefore fit a second model `lm.delivery2`, without `courier_rating_1_to_5`. 

```{r, collapse=TRUE}
# Simplifying the model
# Model 2: Coefficients
lm.delivery2 <- update(lm.delivery1, . ~ . - courier_rating_1_to_5)
lm.delivery2
```


A comparison of the Coefficient estimates between `lm.delivery1` and `lm.delivery2` shows that:

- R-squared and Adjusted R-squared remain unchanged.
- The model quality is practically the same.



#### AIC Comparison: Model 1 vs Model 2

To compare the two models, we use the Akaike Information Criterion (AIC). Lower values indicate a better trade-off between fit and complexity.

```{r}
AIC(lm.delivery1)
AIC(lm.delivery2)
```

Finally, the AIC values of both models are nearly identical, again supporting the simplification.  

**Conclusion:** The model can be reduced without losing explanatory information by removing `courier_rating_1_to_5`.


### 3.1.3 Third Linear Model – including interaction

In the next step, we explore whether an interaction exists between `distance_km` and `traffic_level_factor`.

**Motivation:**  
The effect of distance on delivery time may be different / stronger under heavy traffic conditions. For example, slow progress over a long distance could lead to a much longer delivery times.

We therefore fit a third model including the interaction term:

```r
log_delivery_time ~ distance_km * traffic_level_factor + order_type_factor
```

This model includes:
- Main effects of `distance_km`, `traffic_level_factor`, and `order_type_factor`
- All interaction terms between `distance_km` and `traffic_level_factor`

```{r, collapse=TRUE}
# Including interaction between distance_km and traffic_level_factor
lm.interaction <- lm(log_delivery_time ~ distance_km * traffic_level_factor + order_type_factor, data = train_data)

# Model 3 (interaction): Coefficients
lm.interaction
```

**Model comparison:**

- The **residual standard error** is lower than in the previous model, showing a better fit.
- Both **R-squared** and **Adjusted R-squared** are slightly higher.
- The **F-statistic** from `summary()` is higher for the simpler model `lm.delivery2`, but this is due to fewer parameters (F-stat compares against a null - having no predictors).

To formally compare both models, we further use **AIC** and **ANOVA**.

##### AIC comparison
```{r}
# Compare AIC values
AIC(lm.delivery2)
AIC(lm.interaction)
```

#### ANOVA comparison
```{r}
# anova to compare models
anova(lm.delivery2, lm.interaction)
```

#### Model comparison: `lm.delivery2` vs `lm.interaction`

We use both the Akaike Information Criterion (AIC) and an ANOVA test to compare the simpler model (`lm.delivery2`) with the interaction model (`lm.interaction`).

**AIC comparison:**  
The AIC for `lm.interaction` is **lower** than for `lm.delivery2`, indicating a better fit despite the increased complexity.

**ANOVA comparison:**  
The ANOVA test shows a **significant reduction** in the residual sum of squares when moving from the simpler model to the interaction model.  
- The F-statistic is high.
- The corresponding p-value is very small.

**Conclusion:**  
Both AIC and ANOVA favor the model including the interaction.  
We therefore select `lm.interaction` as our preferred model. This result supports the hypothesis that the effect of distance on delivery time depends on the traffic level.


### 3.1.4 Predictions

After selecting `lm.interaction` as our final model, we now evaluate its performance on the **test dataset**.

We start by generating predictions on the log-scale using the `predict()` function and then transform them back to the original scale (minutes) using `exp()`.

We compare the predicted delivery times to the actual values using scatter plots and vertical error bars to visualize the prediction error.

```{r}
# Prediction, exponentation of prediction and actual delivery time from test_data
predicted_log <- predict(lm.interaction, newdata = test_data)
predicted_min <- exp(predicted_log)
actual_min <- test_data$delivery_time_min
```


```{r, fig.width=12, fig.height = 6}
# Plotting predicted vs. actual delivery time

p1 <- ggplot(test_data, aes(x = predicted_min, y = actual_min)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    title = "Predicted vs Actual Delivery Time",
    x = "Predicted Delivery Time [min]",
    y = "Actual Delivery Time [min]"
  ) +
  theme_minimal()

# Plotting prediction errors on test data

p2 <- ggplot(test_data, aes(x = predicted_min, y = actual_min)) +
  geom_point(alpha = 0.3) +
  geom_segment(aes(xend = predicted_min, yend = predicted_min), color = "blue", alpha = 0.2) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(
    title = "Prediction Errors on Test Data",
    x = "Predicted Delivery Time [min]",
    y = "Actual Delivery Time [min]"
  ) +
  theme_minimal()

p1 + p2
```

```{r}
# calculation root mean squared error and r_squared on test data
rmse <- sqrt(mean((predicted_min - actual_min)^2))
r_squared <- 1 - sum((predicted_min - actual_min)^2) / sum((actual_min - mean(actual_min))^2)
```

```{r}
# calculating rmse for only short deliveries (i.e. < 40min)
results <- data.frame(predicted_min = predicted_min, actual_min = actual_min)
short_deliveries <- results[results$actual_min < 40, ]
rmse_short <- sqrt(mean((short_deliveries$predicted_min - short_deliveries$actual_min)^2))
rmse_short
```

**Observations:**

- For deliveries under **40 minutes**, the predicted values are very close to the actual ones. The model performs well in this range.
- For longer delivery times (above 50 minutes), the **variance increases** and the predictions become less precise.
- A few, rather extreme outliers are present for very long deliveries.
- Overall, the predicted vs actual relationship appears **approximately linear**, and the model performs **robustly**.

We also calculate two common evaluation metrics:

- **Root Mean Squared Error (RMSE):**  
  On the full test set:  
  \[
  \text{RMSE} \approx 5.59 \text{ minutes}
  \]  
On average, the prediction error is around 5.6 minutes.

- **R-squared (on test data):**  
  \[
  R^2 \approx 0.883
  \]  
  → The model explains 88.3% of the variance in delivery times on unseen data, only slightly below the training R², which suggests **no overfitting**.

In addition, we compute the RMSE for **short deliveries (< 40 minutes)** only:
\[
\text{RMSE}_{\text{short}} \approx 3.09 \text{ minutes}
\]  
The model performs **even better** in this range.


## 3.2 Generalized Linear Models

### 3.2.1 Poisson Regression
After first modeling with a linear model, we decided to conduct a further analysis on delivery speed. It is important to understand which variables significantly impact delivery speed: is it the weather conditions, traffic conditions, or even the type of vehicle? The variable `average_speed_kmph` is derived by dividing ``distance_km` by `delivery_time_min`, and then converted to an integer. In this case, `average_speed_kmph` meets the requirement for a Poisson model, which assumes count data as the response variable.

#### 3.2.1.1 Modeling

Before starting the modeling, the `traffic_level_factor` variable is convert to an unordered factor, and its reference level is set to `Very Low`, so that all other traffic levels are compared against this baseline.

In the initial Poisson model, continuous variables such as `courier_age_years`, `temperature_celsius`, `humidity_percent`, `precipitation_mm`, and `distance_km`, as well as categorical variables like `traffic_level_factor`, `vehicle_type_factor`, and `weather_category`, are included in the Poisson model in the first round to examine the validity of these predictors. The goal is to evaluate the significance of these variables in explaining variations in average_speed_kmph.
The results showed in the model summary indicates that the p-values for `courier_age_years`, `precipitation_mm`, and `vehicle_type_factor` are greater than 0.05, suggesting that they do not contribute significantly to the model. Therefore, they will not be considered in the second round of modeling.


```{r, message = FALSE, warning = FALSE, collapse=TRUE}
# read data
df_clean <- read.csv("../data/cleaned_data.csv")


#  Convert to unordered factor
df_clean$traffic_level_factor <- factor(df_clean$traffic_level_factor, ordered = FALSE)
#  Set the reference level
df_clean$traffic_level_factor <- relevel(df_clean$traffic_level_factor, ref = "very low")

# regression with glm poisson model
poisson.model.test <- glm(
  average_speed_kmph ~ courier_age_years + temperature_celsius + humidity_percent + precipitation_mm +
    distance_km + traffic_level_factor + vehicle_type_factor +
    weather_category,
  data = df_clean,
  family = poisson(link="log")
)

# check model summary
summary(poisson.model.test)
```
The second attempt of the Poisson model involves a reduced set of predictors: only `temperature_celsius`, `humidity_percent`, `distance_km`, `traffic_level_factor`, and `weather_category` are included. All remaining predictors show p-values far below 0.05, which indicates all the predictors contribute to the model. 

Another important improvement lies in the ´AIC´ values: the reduced model yields a slightly lower ´AIC´, decreasing from 51224 to 51219, suggesting a better model fit with fewer variables. The Akaike Information Criterion (AIC) is a measure used to compare the quality of statistical models, particularly for models fitted to the same dataset. It helps identify the model that best balances goodness of fit and model simplicity (@wikipedia2025aic).

Additionally, the ´residual deviance´ slightly increases from 7492.1 to 7497.7. While lower deviance generally indicates a better fit (closer to the saturated model), more parameters tend to reduce residual deviance. Therefore, a small increase does not automatically justify a more complex model (@ucla_poisson_residual_deviance; @statology_residual_deviance; @stackexchange_poisson_deviance_df).

Overall, the second Poisson model is more parsimonious and performs better based on the AIC value. The estimated model,expressed in its exponentiated form, is:
 
\[
\begin{aligned}
\text{average_speed_kmph} = \exp(&\, 2.627 
- 0.006 \cdot \text{temperature_celsius} \\
&- 0.002 \cdot \text{humidity_percent} 
+ 0.036 \cdot \text{distance_km} \\
&+ 0.162 \cdot \text{traffic_level_factorhigh}
+ 0.212 \cdot \text{traffic_level_factorlow} \\
&+ 0.275 \cdot \text{traffic_level_factormoderate}
- 0.143 \cdot \text{traffic_level_factorvery high} \\
&- 0.046 \cdot \text{weather_categoryPoorVisibility}
- 0.072 \cdot \text{weather_categoryRainy})
\end{aligned}
\]
\
 
```{r, collapse=TRUE}
# check traffic_level_factor
table(df_clean$traffic_level_factor, useNA = "always")
# Refine the Poisson regression by removing unnecessary variables.
# regression with glm poisson model
poisson.model <- glm(
  average_speed_kmph ~ temperature_celsius + humidity_percent + distance_km + traffic_level_factor +
    weather_category,
  data = df_clean,
  family = poisson(link="log")
)

# check model summary
summary(poisson.model)
```
\[
\begin{aligned}
\text{Log(average_speed_kmph)} = 2.627 &- 0.006 \cdot \text{temperature_celsius} \\
&- 0.002 \cdot \text{humidity_percent} 
+ 0.036 \cdot \text{distance_km} \\
&+ 0.162 \cdot \text{traffic_level_factorhigh}
+ 0.212 \cdot \text{traffic_level_factorlow} \\
&+ 0.275 \cdot \text{traffic_level_factormoderate}
- 0.143 \cdot \text{traffic_level_factorvery high} \\
&- 0.046 \cdot \text{weather_categoryPoorVisibility}
- 0.072 \cdot \text{weather_categoryRainy}
\end{aligned}
\]
\

#### 3.2.1.2 Interpretation of coefficients
Because the Poisson model uses a log-link function, the coefficients are first exponentiated. 

- *Continuous Variables*

The variable ´temperature_celsius´ has an exponentiated coefficient of 0.99, which means that for every 1°C increase in temperature, the expected delivery speed decreases by approximately 1%. This suggests that higher temperatures slightly reduce speed, possibly due to fatigue or decreased performance in hot weather.

The variable ´distance_km´ has an exponentiated coefficient of 1.04. This indicates that for every 1 km increase in distance, the expected delivery speed increases by 4%. Longer delivery distances are associated with slightly higher speeds, possibly because longer routes include faster segments like main roads or highways.

The variable humidity_percent has an exponentiated coefficient of 1.00, indicating no practical effect on delivery speed.

- *Categorical Variables*

The variable traffic_level_factor is an unordered factor with ´Very Low´ as the reference category. ´traffic_level_factorLow´ has an exponentiated coefficient of 1.24, suggesting increase 24% in speed compared to ´Very Low´. ´traffic_level_factorModerate´: 1.32, 32% increase 
´traffic_level_factorHigh´: 1.18, 18% increase
´traffic_level_factorVery High´: 0.87, a 13% decrease in speed. 
These results indicate that mild to moderate traffic conditions are associated with faster delivery speeds, possibly because very low traffic occurs at off-peak hours or in low-demand areas. However, "Very High" traffic reduces speed significantly, as expected.

For the ´weather_category´, ´weather_categoryClear´ was taken as the baseline.
´weather_categoryPoor Visibility´ results exponentialed coefficient 0.96, indicating about 4% decrease in speed. ´weather_categoryRainy´ with exponentialed coefficient 0.93, suggesting 7% decrease comparing to the baseline. These results quantitatively describe how adverse weather conditions reduce delivery speed, with rain having a slightly greater negative effect than poor visibility.


```{r}
#++++++++++++++++++++++++++++++++++++++++++  

names(coef(poisson.model))

#coefficients of the poisson Mode
# first on traffic_level_factor
coef(poisson.model)[c("traffic_level_factorhigh", 
                      "traffic_level_factorlow", 
                      "traffic_level_factormoderate", 
                      "traffic_level_factorvery high")]

exp(coef(poisson.model)[c("traffic_level_factorhigh", 
                          "traffic_level_factorlow", 
                          "traffic_level_factormoderate", 
                          "traffic_level_factorvery high")]) %>% round(digits = 2)

# then on weather_category
coef(poisson.model)[c("weather_categoryPoor Visibility", "weather_categoryRainy")]
exp(coef(poisson.model)[c("weather_categoryPoor Visibility", "weather_categoryRainy")]) %>% round(digits = 2)
# check for continuous variables
coef(poisson.model)[c("temperature_celsius", "humidity_percent","distance_km")]
exp(coef(poisson.model)[c("temperature_celsius", "humidity_percent","distance_km")]) %>% round(digits = 2)
```


#### 3.2.1.3 Comparison of Real vs. Simulated Delivery Speeds by Traffic Level

To assess the adequacy of the Poisson regression model, we simulate delivery speeds using the fitted model and compare them to the actual observed values. Specifically, predicted values of average_speed_kmph are generated via the simulate() function based on the fitted Poisson model. A combined dataset is then constructed that includes both the real and simulated values, with traffic_level_factor as the grouping variable.

The resulting boxplot (Figure X) shows the distribution of delivery speeds for each traffic level, separated by Real (observed) and Simulated (model-generated) speeds.

Overall, the model captures the general trend in delivery speeds across traffic conditions:

Speeds increase from "Very Low" to "Moderate" traffic levels in both real and simulated data, aligning with the previously observed coefficients.

The Simulated values closely follow the center and spread of the Real data, especially under low to moderate traffic conditions.

Slight discrepancies appear under "Very High" traffic, where the model tends to slightly overestimate speed compared to the observed values, reflecting room for model refinement in extreme conditions.

This comparison validates that the Poisson model reasonably replicates key patterns in the data and supports its use for simulating delivery behavior under varying traffic levels.

```{r, fig.align='center'}
# simulate some data from this model
set.seed(2)
sim.data.average_speed_kmph.Poisson <- simulate(poisson.model)
names(sim.data.average_speed_kmph.Poisson) <- "sim_average_speed_kmph"

# Create combined data frame for plotting
df_plot_combined <- bind_rows(
  df_clean %>%
    select(Traffic_Level = traffic_level_factor, speed = average_speed_kmph) %>%
    mutate(Type = "Real"),
  
  data.frame(
    Traffic_Level = df_clean$traffic_level_factor,
    speed = sim.data.average_speed_kmph.Poisson$sim_average_speed_kmph,
    Type = "Simulated"
  )
)

# ensures the plot uses the correct order
df_plot_combined$Traffic_Level <- factor(
  df_plot_combined$Traffic_Level,
  levels = c("very low", "low", "moderate", "high", "very high"),
  ordered = TRUE
)

# Plot grouped boxplot
ggplot(df_plot_combined, aes(x = Traffic_Level, y = speed, fill = Type)) +
  geom_boxplot(outlier.shape = 21, alpha = 0.7, position = position_dodge(width = 0.8)) +
  scale_fill_manual(values = c("Real" = "skyblue", "Simulated" = "lightpink")) +
  labs(
    title = "Figure x: Real vs. Simulated Delivery Speeds by Traffic Level",
    x = "Traffic Level",
    y = "Average Speed (km/h)",
    fill = "Speed Type"
  ) +
  theme_minimal()
```

#### 3.2.1.4 Residual Analysis

To evaluate the adequacy of the Poisson regression model, we conducted a residual analysis using deviance, Pearson, and raw residuals.

**Predicted vs. Deviance Residuals**
We visualized deviance residuals against the predicted delivery speeds to assess model fit. The residuals appear randomly scattered around zero, with no strong systematic patterns, indicating that the model captures the main structure in the data adequately. A slight fan shape at higher predicted values could hint at heteroskedasticity, but the effect is minor.

**Distribution of Deviance Residuals**
We examined the distribution of deviance residuals using a histogram. The distribution is approximately symmetric and centered around zero, which supports the assumption that the model residuals are unbiased and randomly distributed.

While the shape is roughly bell-shaped, there are a few mild outliers (e.g., residuals beyond ±5), but their frequency is very low. This suggests that the Poisson model fits the majority of observations well, no severe skewness or kurtosis is evident in the residuals, outlier influence is limited, and the model is robust to most individual observations.

**Overdispersion Check**
To check for overdispersion, we calculated the overdispersion statistic using Pearson residuals. The resulting overdispersion ratio is 0.83. Since this value is less than 1, it suggests underdispersion — meaning the observed variance in the data is slightly lower than what the Poisson distribution assumes. This is not problematic and indicates that the model may be slightly conservative in its variance estimation but overall remains valid.

**Conclusion**
The residual diagnostics show no major issues with model specification. The model appears to fit the data reasonably well, with no evidence of significant overdispersion or major misspecification.
```{r, fig.width=12, fig.height = 6}

# residual analysis
# 1.compute residuals
df_clean$residuals_raw <- residuals(poisson.model, type = "response")   # raw residuals
df_clean$residuals_deviance <- residuals(poisson.model, type = "deviance")  # deviance residuals
df_clean$residuals_pearson <- residuals(poisson.model, type = "pearson")  # for overdispersion check
df_clean$predicted_speed_kmph <- predict(poisson.model, type = "response") # Compute predicted values

# 2. Plot: Predicted vs. Residuals
p1 <- ggplot(df_clean, aes(x = predicted_speed_kmph, y = residuals_deviance)) +
  geom_point(alpha = 0.4, color = "darkorange") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Deviance Residuals vs. Predicted Speed",
    x = "Predicted Speed (km/h)",
    y = "Deviance Residuals"
  ) +
  theme_minimal()

# 3. Histogram of Residuals
p2 <- ggplot(df_clean, aes(x = residuals_deviance)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "black") +
  labs(
    title = "Histogram of Deviance Residuals",
    x = "Deviance Residuals",
    y = "Count"
  ) +
  theme_minimal()

p1 + p2

# 4. Check for Overdispersion
overdispersion <- sum(residuals(poisson.model, type = "pearson")^2) / poisson.model$df.residual
overdispersion  # > 1 suggests overdispersion; if >>1, Poisson may not be appropriate

```
### 3.2.2 Binomial Regression

<br>

#### Preparation

First step is preprocessing the data further for the binomial model. This includes removing the "bicycle" observations from `vehicle_type_factor` as they are too scarce and could negatively impact on model fitting.

```{r}
#  Convert to unordered factor
df_clean$weather_category <- factor(df_clean$weather_category, ordered = FALSE)
#  Set the reference level
df_clean$weather_category <- relevel(df_clean$weather_category, ref = "Clear")

# Pre-process vehicle_type_factor
df_clean_binomial <- df_clean %>%
  # Remove bicycle as only 10 observations
  filter(vehicle_type_factor != "bicycle")
```

Next, the data set is split into 80% training and 20% testing parts.  

``` {r}
# Split data into 80% train and 20% test
set.seed(42) # starting point for pseudo randomness
train_indices <- createDataPartition(df_clean_binomial$long_delivery_flag, p = 0.8, list = FALSE)
train_data <- df_clean_binomial[train_indices, ]
test_data  <- df_clean_binomial[-train_indices, ]
```

<br>

#### Modelling

As a first attempt all features are included and no interactions are modeled.

``` {r, collapse=TRUE}
# GLM Binomial model (all features)
binomial.model.test <- glm(
  long_delivery_flag ~ courier_age_years + courier_rating_1_to_5 + 
    temperature_celsius + humidity_percent +
    distance_km + order_type_factor + vehicle_type_factor +
    traffic_level_factor + weather_category,
  data = train_data,
  family = binomial
)
summary(binomial.model.test)
```

The standard errors for the parameter estimates of `traffic_level_factor` are way too large. This usually indicates a convergence issue or some degree of data separation. In addition, when fitting the logistic model with this feature, R throws the following error:
`"Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred"`
It turns out that certain classes of `traffic_level_factor` perfectly estimate `long_flag_duration`. This may be seen in the following table.

```{r}
# Complete separation occurs with traffic_level_factor
table(df_clean_binomial$long_delivery_flag, df_clean_binomial$traffic_level_factor)
```

This is problematic as it means that the maximum likelihood estimate (MLE) for the coefficients corresponding to the factor levels of `traffic_level_factor` does not exist and the model fails to converge. As a result, the interpretation of coefficients becomes unstable and meaningless. I decided to progress with a more robust solution: Firth’s correction (bias-reduced logistic regression) implemented with the `brglm2::brglmFit` method.

```{r, collapse=TRUE}
binomial.model.bias.reduced <- glm(
  long_delivery_flag ~ courier_age_years + courier_rating_1_to_5 + 
    temperature_celsius + humidity_percent +
    distance_km + order_type_factor + vehicle_type_factor +
    traffic_level_factor + weather_category,
  data = train_data,
  family = binomial,
  method = "brglmFit"
)
summary(binomial.model.bias.reduced)
```

This method resolved the data separation issue (low standard errors) and decreased the number of Fisher Scoring iterations from 20 to 7. We can see that all features except `courier_age_years`, `courier_rating_1_to_5` `humidity_percent` and `vehicle_type_factor` have a significant p-value. These features are removed in the final model.

```{r, collapse=TRUE}
binomial.model.final <- glm(
  long_delivery_flag ~ temperature_celsius +
    distance_km + order_type_factor +
    traffic_level_factor + weather_category,
  data = train_data,
  family = binomial,
  method = "brglmFit"
)
summary(binomial.model.final)
```

The loss in accuracy of the final model is tested compared to the previous with a Likelihood Ratio Test.

``` {r, collapse=TRUE}
#Likelihood Ratio Test
anova(binomial.model.bias.reduced, binomial.model.final, test="LRT")
```

The null hypothesis in this test is that the simpler model (final model) fits the data as well as the more complex model and the alternative hypothesis that the more complex model provides a significantly better fit. As p-value = 0.4806 (>> 0.05), we fail to reject the null hypothesis and can conclude there was no significant loss in model fit when dropping the aforementioned features. The final model is therefore the better option considering it is more simple.

<br>

#### Evaluation

In order to test the accuracy of the model, predictions are made for the `test_data`, a confusion matrix is created and accuracy is measured.

``` {r}
# Predictions
test_data$y_pred = predict(binomial.model.final,
  test_data[c('temperature_celsius', 'humidity_percent', 'precipitation_mm', 'distance_km', 'order_type_factor',
  'traffic_level_factor', 'weather_category')], type="response")

# Convert to class labels (using 0.5 cutoff)
test_data <- test_data %>%
  mutate(y_pred = if_else(y_pred > 0.5, 1, 0))

# Confusion matrix
table(Predicted = test_data$y_pred, Actual = test_data$long_delivery_flag)

# Accuracy
mean(test_data$y_pred == test_data$long_delivery_flag)
```

The accuracy of the model (with threshold = 0.5) is 93.2% which is very good. 54 deliveries that were shorter than 40 minutes were misclassified as longer than and 69 of deliveries that were longer than 40 minutes were misclassified as shorter than. This corresponds to a specificity of 94.9% and sensitivity of 90.7%. Furthermore, when the model predicts a short delivery, it has 93.6% probability of being correct (negative predictive value) and when it predicts a long delivery, it has a 92.6% probability of being correct.

<br>

#### Interpretation

The estimated model,expressed in its exponentiated form, is:

\[
\begin{aligned}
\text{long_delivery_flag} = \exp(&\, -16.381 
+ 0.084 \cdot \text{temperature_celsius} \\
&+ 0.243 \cdot \text{distance_km} 
+ 7.514 \cdot \text{order_type_factorDrinks} \\
&- 0.719 \cdot \text{order_type_factorMeal}
+ 0.711 \cdot \text{order_type_factorSnack} \\
&- 1.732 \cdot \text{traffic_level_factorlow}
+ 1.792 \cdot \text{traffic_level_factormoderate} \\
&+ 10.219 \cdot \text{traffic_level_factorhigh}
+ 16.605 \cdot \text{traffic_level_factorveryhigh} \\
&+ 0.448 \cdot \text{weather_categoryPoorVisibility}
+ 1.105 \cdot \text{weather_categoryRainy})
\end{aligned}
\]
\

As for the interpretation of these coefficients:

* At reference level: (temperature_celsius = 0, distance_km = 0, order_type_factor = "Buffet", traffic_level_factor = "very low", weather_category = "Clear") the probability of a long delivery is:
\[
P = \frac{e^{-16.4}}{1 + e^{-16.4}} \approx 0
\]

* For each 10 degree increase in temperature, the odds increase by:
\[
P = e^{10 * 0.08359} \approx 2 \text{ times}
\]

* Similarly, for each 10 kilometres increase in distance, the odds increase by approx. 11 times.

* Similarly for a drink instead, the odds increase by:
\[
P = e^{7.51433} \approx 1,834 \text{ times}
\]

* Similarly for a meal instead, the odds decrease by approx. 51%.

* Similarly for a snack instead, the odds increase by approx. 2 times.

* Similarly for low traffic instead, the odds decrease by approx. 82%.

* Similarly for moderate traffic instead, the odds increase by approx. 6 times.

* Similarly for high traffic instead, the odds increase by approx. 27,421 times.

* Similarly for very high traffic instead, the odds increase by approx. 16,278,568 times.

* Similarly for poor visibility instead, the odds increase by approx. 56%.

* Similarly for rainy weather instead, the odds increase by approx. 3 times.

## 3.3. Generalised Additive Models (GAM) 

After analyzing the linear model and the generalized linear model (GLM), we will now explore a more flexible and potentially more powerful approach: the Generalized Additive Model (GAM). Our goal is to improve the prediction of the target variable — `delivery_time_min`. As before, we will use the `df_clean.csv` dataset, which has been cleaned and preprocessed from the raw data. The `gam()` function is found in the `{mgcv}` package. 

### 3.3.1 Modeling

For the GAM modeling, several options can be incorporated into the model: the smoother `s()` function can be applied to continuous variables, and interactions between variables can also be considered. In the linear model, the target variable `delivery_time_min` was log-transformed. Therefore, in the GAM models, both the original and log-transformed versions of the target variable will be compared in terms of their outcomes. Due to space limitations in this document, only three selected models are presented in this chapter, and their results are compared.

**Gam model 1**

First, all continuous and categorical variables are included in an initial GAM to examine their p-values and assess which variables significantly contribute to the model and which do not. The model summary is obtained using `summary(gam.model.test1)`. Since no smooth functions s() included in `gam.model.test1`, it's equivalent to a linear model. 

The first round of results shows that the adjusted R-squared is 0.858. The statistically significant predictors, marked with '***', include the 'Intercept', 'temperature_celsius', 'humidity_percent', 'distance_km', and 'traffic_level_factor', suggesting they have a meaningful impact on delivery time. In contrast, variables such as 'courier_age_years', 'precipitation_mm', 'vehicle_type_factor', and 'weather_category' have p-values greater than 0.05 and therefore do not significantly contribute to the model. These non-significant variables will be excluded in the next modeling steps. 

```{r, warning=FALSE, message=FALSE, collapse=TRUE}
# again loading the cleaned data as df_clean
df_clean <- read.csv("../data/cleaned_data.csv")

# 1st trial gam modeling with all possible variables
gam.model.test1 <- gam(delivery_time_min ~ courier_age_years + temperature_celsius + humidity_percent 
                       + precipitation_mm + distance_km + traffic_level_factor 
                       + vehicle_type_factor + weather_category,
          data = df_clean)
summary(gam.model.test1)
```
**Gam model 2, integrate smoother**

In the second attempt, we include only the statistically significant variables identified previously. Additionally, we apply the `s()` function to all continuous variables — `temperature_celsius`, `humidity_percent`, and `distance_km` — to allow for flexible, non-linear relationships using smooth terms. To avoid overfitting, we use `bs = "cs"` (cubic regression splines with shrinkage), and set `k = 5` to limit the maximum allowable complexity for each smooth term.

Comparing the summary outputs of the two models, the GCV (Generalized Cross-Validation score) decreased from 39.086 to 33.274. GCV is especially useful for GAMs, as it directly relates to prediction error. A lower GCV indicates a better balance between goodness-of-fit and smoothness penalty (@clark_gam, @statisticsglobe_gcv, @crossvalidated_gcv). Additionally, the adjusted R-squared increased from 0.858 to 0.879, further indicating improved model performance.

The anova() function was used to compare the two models. This test assesses whether the more flexible model (Model 2) offers a statistically significant improvement in fit over the simpler one (Model 1). The F-statistic is large (397.3) and the p-value is extremely small (p < 2.2e-16), indicating that the inclusion of smooth functions in Model 2 significantly improves the fit, even though fewer predictors are included. The Df Deviance value, which represents the difference in residual deviance between the two models, is 52,579, again suggesting a better fit.

Lastly, comparing the AIC (Akaike Information Criterion), the value decreased from 58,762 (Model 1) to 57,308 (Model 2). Since lower AIC values indicate a better trade-off between model fit and complexity, this further supports Model 2 as the preferred model.

```{r, collapse=TRUE}
# 2nd Attempt: Model with Selected Variables and Smooth Terms
gam.model.test2 <- gam(delivery_time_min ~ s(temperature_celsius, bs = "cs", k=5) + 
                         s(humidity_percent, bs = "cs", k=5) + 
                         s(distance_km, bs = "cs", k=5) + 
                         traffic_level_factor + 
                         weather_category,
                       data = df_clean) 
# print the summary
summary(gam.model.test2)

# model comparing test1 and test2
anova(gam.model.test1, gam.model.test2, test = "F")
AIC(gam.model.test1, gam.model.test2)

# Create data frame
model_comparison <- data.frame(
  MODEL = c("gam.model.test1", "gam.model.test2"),
  AIC = c(58762.26, 57307.94),
  GCV = c(39.086, 33.274),
  R2 = c(0.858, 0.879)
)

# Print the table
kable(model_comparison, digits = 2, align = 'c', caption = "Model Comparison: gam.model.test1 vs gam.model.test2")
```

**Gam model 3, integrate interactions**

In the next step, interactions between predictors will be analyzed and selected for modeling. After testing several trial models, we compare two interaction candidates to evaluate their improvement over the existing model, `gam.model.test2`. The interactions `temperature_celsius:humidity_percent` and `distance_km:traffic_level_factor` are included in `gam.model.test6`, while only `temperature_celsius:humidity_percent` is included in `gam.model.test8`. Their performance is compared using the same criteria: anova(), AIC, and GCV values.

Two generalized additive models (GAMs) were compared to assess the effect of including interaction terms. `gam.model.test6` includes both interactions `temperature_celsius: humidity_percent` and `distance_km:traffic_level_factor`, while `gam.model.test8` includes only the first interaction.

`gam.model.test6` has a slightly lower AIC (57247.91 vs. 57264.50) and marginally better GCV (33.05 vs. 33.12), indicating a minor improvement in model fit. Both models achieve the same R² value of 0.88, suggesting equal explanatory power.

The interaction `temperature_celsius:humidity_percent` is statistically significant (p < 0.001) and suggests that higher temperature combined with higher humidity slightly decreases delivery time.

All interaction terms involving `distance_km:traffic_level_factor` in `gam.model.test6` are not statistically significant (p > 0.23), implying they do not contribute meaningful information.

Smooth term plots are consistent across models: temperature and humidity show modest nonlinear effects, while distance has a stronger positive effect in `gam.model.test8`.

Although `gam.model.test6` fits slightly better, the improvement is minimal, and the added complexity from the insignificant distance_km:traffic_level_factor interaction is not justified.

Therefore, `gam.model.test8` is preferable for its simplicity and comparable performance. It retains the significant interaction and avoids unnecessary terms that may lead to overfitting.

```{r, collapse=TRUE, out.width = "100%", fig.height = 2}
# 6th fitting, include recommended interations to the GAM with shrinkage of cyclic spline
gam.model.test6 <- gam(delivery_time_min ~ s(temperature_celsius, bs = "cs", k=5) + 
                                           s(humidity_percent, bs = "cs", k=5) + 
                                           s(distance_km, bs = "cs", k=5) + 
                                           traffic_level_factor + 
                                           weather_category +
                                           temperature_celsius:humidity_percent +
                                           distance_km:traffic_level_factor,
                                           data = df_clean)
summary(gam.model.test6)
#  visualize the smooth terms, Plot all smooth terms with confidence intervals 
# Set layout to 1 row, 3 columns
par(mfrow = c(1, 3))

# Plot each smooth term individually
plot(gam.model.test6, select = 1, se = TRUE, rug = TRUE, main = "Test6: Temp Smooth Term")
plot(gam.model.test6, select = 2, se = TRUE, rug = TRUE, main = "Test6: Humidity Smooth Term")
plot(gam.model.test6, select = 3, se = TRUE, rug = TRUE, main = "Test6: Distance Smooth Term")

#plot(gam.model.test6, pages = 1, se = TRUE, rug = TRUE, main="Smooth Terms of GAM Model Test 6")

# 8th continue after 6th, remove insignificant variables
gam.model.test8 <- gam(delivery_time_min ~ s(temperature_celsius, bs = "cs", k=5) + 
                         s(humidity_percent, bs = "cs", k=5) + 
                         s(distance_km, bs = "cs", k=5) + 
                         traffic_level_factor + 
                         weather_category +
                         temperature_celsius:humidity_percent,
                       data = df_clean)
summary(gam.model.test8)
#  visualize the smooth terms, Plot all smooth terms with confidence intervals 
#plot(gam.model.test8, pages = 1, se = TRUE, rug = TRUE, main="Smooth Terms of GAM Model Test 8")


# Plot each smooth term individually
plot(gam.model.test8, select = 1, se = TRUE, rug = TRUE, main = "Test8: Temp Smooth Term")
plot(gam.model.test8, select = 2, se = TRUE, rug = TRUE, main = "Test8: Humidity Smooth Term")
plot(gam.model.test8, select = 3, se = TRUE, rug = TRUE, main = "Test8: Distance Smooth Term")

# model comparing test1 and test2
anova(gam.model.test6, gam.model.test8, test = "F")
AIC(gam.model.test6, gam.model.test8)


# Create data frame
model_comparison <- data.frame(
  MODEL = c("gam.model.test6", "gam.model.test8"),
  AIC = c(57247.91, 57264.50),
  GCV = c(33.054, 33.115),
  R2 = c(0.88, 0.879)
)

# Print the table
kable(model_comparison, digits = 2, align = 'c', caption = "Model Comparison: gam.model.test6 vs gam.model.test8")
```

**Gam model 4, integrate log transformation on target variable**
In the linear model, the target variable delivery_time_min was log-transformed to improve model performance (see Section 3.1). We apply the same transformation to gam.model.test8, resulting in gam.model.logtest8, and compare the performance to the untransformed version.

The model is fitted using the log-transformed delivery time (log_delivery_time) as the response variable while retaining the same predictors and interaction term (temperature_celsius:humidity_percent).

The model achieves a high adjusted R² of 0.886, indicating that 88.6% of the variance in log-transformed delivery time is explained by the model. The GCV score is 0.0255, showing a low level of prediction error, which suggests `gam.model.logtest8` is a robust choice for modeling delivery time.

```{r, collapse=TRUE}
# Adding log transformed delivery time to d.food_time
df_clean <- df_clean %>%
  mutate(log_delivery_time = log(delivery_time_min))
# 8th continue after 6th, remove insignificant variables
gam.model.logtest8 <- gam(log_delivery_time ~       s(temperature_celsius, bs = "cs", k=5) + 
                         s(humidity_percent, bs = "cs", k=5) + 
                         s(distance_km, bs = "cs", k=5) + 
                         traffic_level_factor + 
                         weather_category +
                         temperature_celsius:humidity_percent,
                       data = df_clean)
summary(gam.model.logtest8)
```


### 3.3.2 Model Comparison and Evaluation

To evaluate the performance of different modeling approaches for predicting delivery time, three models were compared using RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) on a held-out test set:

-- Linear Model (`gam.model.test1`) – A simple GAM without smoothing terms, treating all predictors as linear.

-- GAM on Original Scale (`gam.model.test8`) – A flexible GAM using cubic regression splines (bs = "cs") for numeric predictors and including an interaction between temperature and humidity.

-- GAM with Log-Transformed Target (`gam.model.logtest8`) – Same as test8, but applied to a log-transformed delivery time to account for potential skewness and stabilize variance.

The results show:

The log-transformed GAM model achieved the lowest RMSE (5.69) and lowest MAE (4.24), indicating the best predictive accuracy overall. The GAM on the original scale (test8) also performed better than the linear model, suggesting that including non-linear effects improves model fit. The linear model (test1) showed the highest error rates, confirming that linear assumptions are too restrictive for this problem.

Overall, the results suggest that: Applying a log transformation to the target variable can further improve performance, likely due to reducing the influence of outliers and heteroscedasticity. These findings support the use of flexible models such as GAMs, especially when paired with appropriate transformations of the response variable.

```{r}
#  ---- For linear model test  ----
set.seed(99)
train_index <- createDataPartition(df_clean$delivery_time_min, p = 0.8, list = FALSE)
train_data <- df_clean[train_index, ]
test_data <- df_clean[-train_index, ]

gam.model.traintest1 <- gam(delivery_time_min ~ 
                              temperature_celsius + 
                              humidity_percent + 
                              distance_km + 
                              traffic_level_factor + 
                              weather_category, 
                            data = train_data,
                            method = "REML")
                              
pred_test1 <- predict(gam.model.traintest1, newdata = test_data)

RMSE_test1 <- sqrt(mean((pred_test1 - test_data$delivery_time_min)^2))
MAE_test1 <- mean(abs(pred_test1 - test_data$delivery_time_min))


# ---- For test8  ----
set.seed(100)
train_index <- createDataPartition(df_clean$delivery_time_min, p = 0.8, list = FALSE)
train_data <- df_clean[train_index, ]
test_data <- df_clean[-train_index, ]

gam.model.traintest8 <- gam(delivery_time_min ~
                              s(temperature_celsius, bs = "cs", k=5) + 
                              s(humidity_percent, bs = "cs", k=5) + 
                              s(distance_km, bs = "cs", k=5) + 
                              traffic_level_factor + 
                              weather_category +
                              temperature_celsius:humidity_percent,
                            data = train_data,
                            method = "REML")

pred_test8 <- predict(gam.model.traintest8, newdata = test_data)

RMSE_test8 <- sqrt(mean((pred_test8 - test_data$delivery_time_min)^2))
MAE_test8 <- mean(abs(pred_test8 - test_data$delivery_time_min))


# ---- For log transformation logtest8 ----
set.seed(101)
train_index <- createDataPartition(df_clean$log_delivery_time, p = 0.8, list = FALSE)
train_data <- df_clean[train_index, ]
test_data <- df_clean[-train_index, ]

gam.model.trainlogtest8 <- gam(log_delivery_time ~
                                 s(temperature_celsius, bs = "cs", k=5) + 
                                 s(humidity_percent, bs = "cs", k=5) + 
                                 s(distance_km, bs = "cs", k=5) + 
                                 traffic_level_factor + 
                                 weather_category +
                                 temperature_celsius:humidity_percent,
                               data = train_data,
                               method = "REML")

pred_logtest8 <- predict(gam.model.trainlogtest8, newdata = test_data)
pred_logtest8_back <- exp(pred_logtest8)

RMSE_logtest8 <- sqrt(mean((pred_logtest8_back - test_data$delivery_time_min)^2))
MAE_logtest8 <- mean(abs(pred_logtest8_back - test_data$delivery_time_min))


# ---- Model Comparison Table ----
data.frame(
  Model = c("Linear model (test 1)", "Original Scale (test8)", "Log-Transformed (logtest8)"),
  RMSE = c(RMSE_test1, RMSE_test8, RMSE_logtest8),
  MAE = c(MAE_test1, MAE_test8, MAE_logtest8)
)


```

**visualization**


```{r, out.width = "100%", fig.height = 2}
# Predicted values
df_clean$pred_lm      <- predict(gam.model.traintest1, newdata = df_clean)
df_clean$pred_gam     <- predict(gam.model.traintest8, newdata = df_clean)
df_clean$pred_log_gam <- exp(predict(gam.model.trainlogtest8, newdata = df_clean))

# Residuals
df_clean$resid_lm      <- df_clean$delivery_time_min - df_clean$pred_lm
df_clean$resid_gam     <- df_clean$delivery_time_min - df_clean$pred_gam
df_clean$resid_log_gam <- df_clean$delivery_time_min - df_clean$pred_log_gam

# ----- Residual Plots -----
par(mfrow = c(1, 3))

plot(df_clean$resid_lm, 
     main = "Residuals: Linear Model", 
     ylab = "Residuals", xlab = "Index", col = "darkgreen", pch = 20)

plot(df_clean$resid_gam, 
     main = "Residuals: Smooth GAM", 
     ylab = "Residuals", xlab = "Index", col = "blue", pch = 20)

plot(df_clean$resid_log_gam, 
     main = "Residuals: Log-Transformed GAM", 
     ylab = "Residuals", xlab = "Index", col = "purple", pch = 20)


# ----- Predicted vs Actual -----
par(mfrow = c(1, 3))

plot(df_clean$delivery_time_min, df_clean$pred_lm,
     main = "Linear Model",
     xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0, 0.6, 0, 0.3))
abline(0, 1, col = "red", lwd = 2)

plot(df_clean$delivery_time_min, df_clean$pred_gam,
     main = "Smooth GAM (Original)",
     xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0, 0, 1, 0.3))
abline(0, 1, col = "red", lwd = 2)

plot(df_clean$delivery_time_min, df_clean$pred_log_gam,
     main = "Log-Transformed GAM",
     xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0.5, 0, 0.5, 0.3))
abline(0, 1, col = "red", lwd = 2)
```

```{r, include=FALSE}
# Reset layout
par(mfrow = c(1, 1))
```

## 3.4 Support Vector Machine

In this section, we aim to classify the **traffic level** using SVM models.

### 3.4.1 Data Preparation

We begin with the following steps:

- Create a new dataset `svm_data` with relevant features
- Ensure the target variable is a factor
- Split the data into a training and a test set using **stratified sampling**


```{r}
# Create input data for SVM
svm_data <- df_clean %>%
  select(traffic_level_factor, distance_km, average_speed_kmph)

# Ensure target variable is a factor
svm_data$traffic_level_factor <- factor(svm_data$traffic_level_factor)

# Stratified sampling for training and test set
set.seed(123)
split_index <- createDataPartition(svm_data$traffic_level_factor, p = 0.8, list = FALSE)

train_svm <- svm_data[split_index, ]
test_svm  <- svm_data[-split_index, ]
```

### 3.4.2 First SVM Model – Linear Kernel

We start with a basic SVM model using:

- `distance_km`
- `average_speed_kmph`

We apply a **linear kernel** with a `cost` parameter of 10.

To avoid overfitting, we scale the input variables and use the `e1071::svm()` function. Later, we will compare this model to a radial SVM and explore parameter tuning.

```{r, collapse=TRUE}
# Train SVM with linear kernel
svm_model1 <- svm(
  traffic_level_factor ~ distance_km + average_speed_kmph,
  data = train_svm,
  method = "C-classification",
  kernel = "linear",
  cost = 10,
  scale = TRUE
)

# Predict on test set
pred_svm <- predict(svm_model1, newdata = test_svm)

# Evaluate classification performance
confusionMatrix(pred_svm, test_svm$traffic_level_factor)
```
#### Model Evaluation – Linear SVM

We evaluate the prediction accuracy using a confusion matrix.

**Results:**

- **Accuracy:** 84.5% of the test set was correctly classified
- **95% CI:** Approximately 82.7% to 86.1%
- **Significance:** The model performs significantly better than random guessing or choosing the most frequent class

However, the model struggles to distinguish between `"moderate"` and `"high"` traffic levels:
- 45 `"moderate"` observations were misclassified as `"high"`
- 48 `"high"` observations were misclassified as `"moderate"`

This pattern is also reflected visually in the decision boundaries (see next section).

### 3.4.3 Visualizing SVM Decision Boundaries (Linear Kernel)

To better understand how the SVM model separates traffic levels, we visualize the decision boundaries in the feature space defined by:

- `distance_km`
- `average_speed_kmph`

We generate a prediction grid and overlay it with the training points to illustrate the model's classification behavior.

```{r, fig.align='center'}
# Create grid of points across the feature space
xrange <- seq(min(svm_data$distance_km), max(svm_data$distance_km), length.out = 200)
yrange <- seq(min(svm_data$average_speed_kmph), max(svm_data$average_speed_kmph), length.out = 200)
grid <- expand.grid(distance_km = xrange, average_speed_kmph = yrange)

# Predict traffic level on grid
grid$predicted <- predict(svm_model1, newdata = grid)

# Plot decision boundaries with actual training points
ggplot() +
  geom_tile(data = grid, aes(x = distance_km, y = average_speed_kmph, fill = predicted), alpha = 0.3) +
  geom_point(data = train_svm, aes(x = distance_km, y = average_speed_kmph, color = traffic_level_factor), alpha = 0.7, size = 1.2) +
  scale_fill_viridis_d(option = "plasma", name = "Predicted Class") +
  scale_color_viridis_d(option = "plasma", name = "True Class") +
  labs(
    title = "SVM Decision Boundaries (Linear Kernel)",
    x = "Distance [km]",
    y = "Average Speed [km/h]"
  ) +
  theme_minimal()
```

#### Interpretation

The decision regions show that the model draws clear boundaries between traffic levels. However, the overlap between `"moderate"` and `"high"` levels is visually noticeable — this confirms the misclassifications seen in the confusion matrix.

The linear kernel creates **linear boundaries**, which may not fully capture the complexity of the relationships in the data. Therefore, we next try a **radial kernel** (nonlinear).

### 3.4.4 Second SVM Model – Radial Kernel

We now fit a second model using a **radial basis function (RBF)** kernel. This allows for nonlinear decision boundaries, potentially improving performance in cases where classes overlap in complex ways.

We use the same cost parameter as before (`cost = 10`) and set `gamma = 0.1`. Both variables are scaled.

```{r, collapse=TRUE}
# Train SVM with radial kernel
svm_model2 <- svm(
  traffic_level_factor ~ distance_km + average_speed_kmph,
  data = train_svm,
  kernel = "radial",
  cost = 10,
  gamma = 0.1,
  scale = TRUE
)

# Predict on test set
pred_rbf <- predict(svm_model2, newdata = test_svm)

# Evaluate performance
confusionMatrix(pred_rbf, test_svm$traffic_level_factor)
```
#### Model Evaluation – Radial SVM

The confusion matrix for the radial SVM shows similar performance to the linear model.

**Observation:**
- The overall accuracy is comparable to the linear model.
- The same confusion between `"moderate"` and `"high"` levels persists.

This suggests that, while the radial kernel introduces nonlinear decision boundaries, it does **not significantly improve** classification performance in this case.

#### Visualizing Decision Boundaries – Radial SVM

We now plot the decision boundaries of the **radial SVM** using the same feature grid as before. Unlike the linear SVM, the radial kernel allows for **nonlinear and curved** boundaries.

This helps us visually assess whether the model better separates overlapping classes.

```{r, fig.align='center'}
# Create prediction grid (as before)
xrange <- seq(min(svm_data$distance_km), max(svm_data$distance_km), length.out = 200)
yrange <- seq(min(svm_data$average_speed_kmph), max(svm_data$average_speed_kmph), length.out = 200)
grid <- expand.grid(distance_km = xrange, average_speed_kmph = yrange)

# Predict class for each point using the radial SVM
grid$predicted <- predict(svm_model2, newdata = grid)

# Plot decision boundaries and training points
ggplot() +
  geom_tile(data = grid, aes(x = distance_km, y = average_speed_kmph, fill = predicted), alpha = 0.3) +
  geom_point(data = train_svm, aes(x = distance_km, y = average_speed_kmph, color = traffic_level_factor), alpha = 0.7, size = 1.2) +
  scale_fill_viridis_d(option = "plasma", name = "Predicted Class") +
  scale_color_viridis_d(option = "plasma", name = "True Class") +
  labs(
    title = "SVM Decision Boundaries (Radial Kernel)",
    x = "Distance [km]",
    y = "Average Speed [km/h]"
  ) +
  theme_minimal()
```

#### Interpretation

The decision boundaries created by the **radial kernel** are more flexible and curved compared to the linear model. However, there is still noticeable overlap between `"moderate"` and `"high"` traffic levels.

This confirms what we observed in the confusion matrix: even with a nonlinear kernel, the model still struggles in this specific region of the feature space.

Next, we attempt to improve the model via **parameter tuning** using grid search and cross-validation.

### 3.4.5 SVM Parameter Tuning – Grid Search with Cross-Validation

To potentially improve the model performance, we now perform a **grid search** for the optimal combination of `cost` and `gamma` values.

We use `e1071::tune()` with 10-fold cross-validation to evaluate combinations of:

- `cost`: 0.1, 1, 10, 100
- `gamma`: 0.01, 0.05, 0.1, 0.5

This process helps identify the parameter settings that yield the highest accuracy on the training data.

```{r, cache = TRUE, eval=FALSE, collapse=TRUE}
set.seed(123)  # starting point pseudo randomness

# Grid search using cross-validation
tuned_model <- tune(
  svm,
  traffic_level_factor ~ distance_km + average_speed_kmph,
  data = train_svm,
  kernel = "radial",
  ranges = list(
    cost = c(0.1, 1, 10, 100),
    gamma = c(0.01, 0.05, 0.1, 0.5)
  )
)

# Print summary of tuning results
summary(tuned_model)
```
#### Tuning Results

The output lists all combinations of `cost` and `gamma` along with their cross-validated accuracy.  
The combination with the **highest accuracy** is automatically selected as the best model.

We now extract this model and evaluate it on the **test dataset**.

```{r, eval=FALSE}
# Extract best model
best_model <- tuned_model$best.model

# Make predictions on test set
pred_tuned <- predict(best_model, newdata = test_svm)

# Confusion matrix for tuned model
confusionMatrix(pred_tuned, test_svm$traffic_level_factor)
```

#### Final Evaluation – Tuned Radial SVM

The confusion matrix for the best model shows that the **overall performance is very similar** to the untuned radial SVM.

**Conclusion:**

- No notable improvement through tuning
- The initial radial model already performed well
- The confusion between `"moderate"` and `"high"` traffic levels remains the main source of error
- This confirms that SVMs can be robust even with default or simple parameter choices

In summary, the SVM models – particularly with the radial kernel – perform well in classifying traffic levels, though some overlaps between classes remain difficult to resolve with only two features.


## 3.5 Neural Network


# 4. Results and Discussion

Presentation of model performances and comparison.

# 5. Conclusion

Interpretation of findings and implications.

# 6 Chapter on AI 

1- How did we use it? 

2- Where did we have to be careful etc. 

# References
